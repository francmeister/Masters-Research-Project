{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Masters-Research-Project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Cloning Github Repository and navigate into project folder\n",
    "!git clone https://github.com/francmeister/Masters-Research-Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\OneDrive - University of Witwatersrand\\Desktop\\Francis\\Masters-Research-Project\\Multi-User-MEC-System\n"
     ]
    }
   ],
   "source": [
    "cd Masters-Research-Project/Multi-User-MEC-System/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAHMB0Ze8fU0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Administrator/OneDrive%20-%20University%20of%20Witwatersrand/Desktop/Francis/Masters-Research-Project/Multi-User-MEC-System/Network_Env\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (from Network-Env==0.0.1) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym->Network-Env==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym->Network-Env==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym->Network-Env==0.0.1) (0.0.8)\n",
      "Installing collected packages: Network-Env\n",
      "  Attempting uninstall: Network-Env\n",
      "    Found existing installation: Network-Env 0.0.1\n",
      "    Uninstalling Network-Env-0.0.1:\n",
      "      Successfully uninstalled Network-Env-0.0.1\n",
      "  Running setup.py develop for Network-Env\n",
      "Successfully installed Network-Env-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e Network_Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\OneDrive - University of Witwatersrand\\Desktop\\Francis\\Masters-Research-Project\\Multi-User-MEC-System\\Network_Env\n"
     ]
    }
   ],
   "source": [
    "cd Network_Env/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from NetworkEnv_ import NetworkEnv_\n",
    "import Network_Env\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pygame\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "#from google.colab import files\n",
    "from numpy import interp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.sigmoid(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "    # self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    # self.layer_2 = nn.Linear(400, 1)\n",
    "\n",
    "    # self.layer_3 = nn.Linear(state_dim + action_dim, 400)\n",
    "    # self.layer_4 = nn.Linear(400, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    #self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.000001)\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.00001)\n",
    "    \n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.0001)\n",
    "    #self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.00001)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    #return self.actor(state).cpu().data.numpy().flatten()\n",
    "    return self.actor(state).cpu().data.numpy()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=60, discount=0.99, tau=0.005, policy_noise=0.3, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "      #batch_states = np.reshape(batch_states,(batch_states.shape[0]*batch_states.shape[1],batch_states.shape[2]))\n",
    "      #batch_next_states = np.reshape(batch_next_states,(batch_next_states.shape[0]*batch_next_states.shape[1],batch_next_states.shape[2]))\n",
    "      #batch_actions = np.reshape(batch_actions,(batch_actions.shape[0]*batch_actions.shape[1],batch_actions.shape[2]))\n",
    "\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We build respective classes for DDPG implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One DDPG Actor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_DDPG(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor_DDPG, self).__init__()\n",
    "\n",
    "\t\tself.layer_1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.layer_2 = nn.Linear(400, 300)\n",
    "\t\tself.layer_3 = nn.Linear(300, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\n",
    "\t\n",
    "\tdef forward(self, state):\n",
    "\t\tx = F.relu(self.layer_1(state))\n",
    "\t\tx = F.relu(self.layer_2(x))\n",
    "\t\treturn self.max_action * torch.sigmoid(self.layer_3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One DDPG Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_DDPG(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic_DDPG, self).__init__()\n",
    "\n",
    "\t\t#self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "\t\t#self.layer_2 = nn.Linear(400, 300)\n",
    "\t\tself.layer_1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.layer_2 = nn.Linear(400 + action_dim, 300)\n",
    "\t\tself.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\t#q = F.relu(self.layer_1(torch.cat([state, action], 1)))\n",
    "\t\t#q = F.relu(self.layer_2(q))\n",
    "\n",
    "\t\tq = F.relu(self.layer_1(state))\n",
    "\t\tq = F.relu(self.layer_2(torch.cat([q, action], 1)))\n",
    "\t\treturn self.layer_3(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class DDPG(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tself.actor = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.000001)\n",
    "\n",
    "\t\tself.critic = Critic_DDPG(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = Critic_DDPG(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.0001)\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\t#state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\tstate = torch.Tensor(state).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy()\n",
    "\t\t#return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\tdef train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\t\t# Sample replay buffer \n",
    "\t\tbatch_states, batch_next_states, batch_actions,batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "\t\t\n",
    "\t\tstate = torch.Tensor(batch_states).to(device)\n",
    "\t\tnext_state = torch.Tensor(batch_next_states).to(device)\n",
    "\t\taction = torch.Tensor(batch_actions).to(device)\n",
    "\t\treward = torch.Tensor(batch_rewards).to(device)\n",
    "\t\tdone = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "\n",
    "\t\t# Compute the target Q value\n",
    "\t\tnext_action = self.actor_target(next_state)\t\n",
    "\t\ttarget_Q = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "\t\t#target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "\t\ttarget_Q = reward + ((1 - done) *discount * target_Q).detach()\n",
    "\n",
    "\t\t# Get current Q estimate\n",
    "\t\tcurrent_Q = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Compute actor loss\n",
    "\t\tactor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\t\t\n",
    "\t\t# Optimize the actor \n",
    "\t\tself.actor_optimizer.zero_grad()\n",
    "\t\tactor_loss.backward()\n",
    "\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t# Update the frozen target models\n",
    "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename, directory):\n",
    "\t\ttorch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "\t\ttorch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "\tdef load(self, filename, directory):\n",
    "\t\tself.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "\t\tself.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=1):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(obs)\n",
    "      action = env.reshape_action_space_from_model_to_dict(action)\n",
    "      mode = 'training'\n",
    "      reformed_action = env.apply_resource_allocation_constraint(action,mode)\n",
    "      obs, reward, done, _ = env.step(reformed_action)\n",
    "      avg_reward += reward#interp(sum(reward),[720000000,863000000],[0,1000])\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"NetworkEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "policy_type = 'TD3'\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 10000 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5000 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 250000 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.3 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 60 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.4 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fyH8N5z-o3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_NetworkEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "file_name_1 = \"timestep_rewards_energy_throughput\"\n",
    "file_name_2 = \"offloading_actions\"\n",
    "file_name_3 = \"power_actions\"\n",
    "file_name_4 = \"subcarrier_actions\"\n",
    "file_name_5 = \"allocated_RBs\"\n",
    "file_name_6 = \"fairnes_index\"\n",
    "\n",
    "file_name_7 = \"energy_efficiency_rewards\"\n",
    "file_name_8 = \"battery_energy_rewards\"\n",
    "file_name_9 = \"throughput_rewards\"\n",
    "file_name_10 = \"delay_rewards\"\n",
    "file_name_11 = \"sum_allocations_per_RB_matrix\"\n",
    "file_name_12 = \"RB_allocation_matrix\"\n",
    "file_name_13 = \"energy_rewards\"\n",
    "file_name_14 = \"delays\"\n",
    "file_name_15 = \"tasks_dropped\"\n",
    "file_name_16 = \"outage_probabilties\"\n",
    "file_name_17 = \"resource_allocation_constraint_violation_count\"\n",
    "file_name_18 = \"urllc_reliability_reward\"\n",
    "\n",
    "file_name_19 = \"individual_energy_rewards\"\n",
    "file_name_20 = \"individual_channel_rate_rewards\"\n",
    "file_name_21 = \"individual_channel_battery_energy_rewards\"\n",
    "file_name_22 = \"individual_delay_rewards\"\n",
    "file_name_23 = \"individual_queue_delays\"\n",
    "file_name_24 = \"individual_tasks_dropped\"\n",
    "file_name_25 = \"individual_energy_efficiency\"\n",
    "file_name_26 = \"individual_total_reward\"\n",
    "file_name_27 = \"total reward\"\n",
    "file_name_28 = \"overall_users_reward\"\n",
    "file_name_29 = \"q_action\"\n",
    "file_name_30 = \"RB_bandwidth\"\n",
    "file_name_31 = \"rate_variance\"\n",
    "file_name_32 = \"inf_total_reward\"\n",
    "file_name_33 = \"inf_energy\"\n",
    "file_name_34 = \"inf_throughput\"\n",
    "file_name_35 = \"inf_fairness_index\"\n",
    "file_name_36 = \"inf_task_delay\"\n",
    "file_name_37 = \"urllc_avg_rate\"\n",
    "file_name_38 = \"individual_channel_rates\"\n",
    "file_name_39 = \"individual_local_queue_delays\"\n",
    "file_name_40 = \"individual_offload_queue_delays\"\n",
    "file_name_41 = \"individual_local_queue_lengths\"\n",
    "file_name_42 = \"individual_offload_queue_lengths\"\n",
    "file_name_43 = \"users_lc_service_rates\"\n",
    "file_name_44 = \"resource_block_action_matrix\"\n",
    "file_name_45 = \"individual_expected_rate_over_prev_T_slot\"\n",
    "file_name_46 = \"individual_average_task_size_offload_queue\"\n",
    "\n",
    "file_name_47 = \"individual_battery_energy_levels\"\n",
    "file_name_48 = \"individual_energy_harvested\"\n",
    "file_name_49 = \"throughput_log_reward\"\n",
    "file_name_50 = \"individual_local_energy_consumed\"\n",
    "file_name_51 = \"individual_offloading_energy\"\n",
    "file_name_52 = \"individual_small_scale_gains\"\n",
    "file_name_53 = \"individual_large_scale_gains\"\n",
    "file_name_54 = \"individual_average_offloading_rates\"\n",
    "file_name_55 = \"individual_local_queue_length_num_tasks\"\n",
    "file_name_56 = \"individual_offload_queue_length_num_tasks\"\n",
    "file_name_57 = \"individual_offload_stability_constraint_reward\"\n",
    "file_name_58 = \"total_offload_traffic_reward\"\n",
    "file_name_59 = \"individual_offload_traffic_numerator\"\n",
    "file_name_60 = \"individual_local_queueing_violation_prob_reward\"\n",
    "file_name_61 = \"individual_offload_ratio_reward\"\n",
    "file_name_62 = \"total_local_queueing_violation_prob_reward\"\n",
    "file_name_63 = \"total_offload_ratio_reward\"\n",
    "file_name_64 = \"urllc_total_rate\"\n",
    "file_name_65 = \"F_L_inverse\"\n",
    "file_name_66 = \"urllc_total_rate_per_second\"\n",
    "file_name_67 = \"urllc_total_rate_per_slot\"\n",
    "file_name_68 = \"individual_urllc_channel_rate_per_slot_with_penalty\"\n",
    "file_name_69 = \"individual_urllc_channel_rate_per_second_penalties\"\n",
    "file_name_70 = \"individual_urllc_channel_rate_per_second_without_penalty\"\n",
    "file_name_71 = \"individual_urllc_channel_rate_per_second_with_penalty\"\n",
    "file_name_72 = \"individual_embb_puncturing_users_sum_data_rates\"\n",
    "file_name_73 = \"individual_embb_num_puncturing_users\"\n",
    "file_name_74 = \"failed_urllc_transmissions\"\n",
    "file_name_75 = \"sum_qeueing_latencies\"\n",
    "file_name_76 = \"individual_simulation_total_delay\"\n",
    "file_name_77 = \"individual_simulation_offload_queueing_delay\"\n",
    "file_name_78 = \"individual_simulation_local_queueing_delay\"\n",
    "file_name_79 = \"total_local_traffic_reward\"\n",
    "file_name_80 = \"total_offload_queueing_violation_prob_reward\"\n",
    "\n",
    "\n",
    "\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if not os.path.exists(\"./inference_results\"):\n",
    "  os.makedirs(\"./inference_results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "#env = NetworkEnv_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3RufYec_ADj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action sapce dim:  155\n"
     ]
    }
   ],
   "source": [
    "#env.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#np.random.seed(seed)\n",
    "state_dim = env.observation_space_dim\n",
    "action_dim = env.action_space_dim\n",
    "max_action = float(env.box_action_space.high[0][1]) # to change this soon\n",
    "print('action sapce dim: ', action_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "if policy_type == 'TD3':\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "elif policy_type == 'DDPG':\n",
    "    policy = DDPG(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhC_5XJ__Orp"
   },
   "outputs": [],
   "source": [
    "#evaluations = [evaluate_policy(policy)]\n",
    "evaluations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env.STEP_LIMIT\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()\n",
    "timestep_rewards = []\n",
    "timestep_rewards_energy_throughput_delays = []\n",
    "offload_actions = []\n",
    "power_actions = []\n",
    "subcarrier_actions = []\n",
    "allocated_RBs = []\n",
    "fairness_index = []\n",
    "energy_efficiency_rewards = []\n",
    "battery_energy_rewards = []\n",
    "energy_rewards = []\n",
    "throughput_rewards = []\n",
    "delay_rewards = []\n",
    "sum_allocations_per_RB_matrix = []\n",
    "change_action = 0\n",
    "RB_allocation_matrix = []\n",
    "delays = []\n",
    "urllc_reliability_reward = []\n",
    "tasks_dropped = []\n",
    "resource_allocation_matrix = []\n",
    "resource_allocation_constraint_violation_count = []\n",
    "outage_probabilties = []\n",
    "urllc_reliability_reward_normalized = []\n",
    "individual_energy_rewards = []\n",
    "individual_channel_rate_rewards = []\n",
    "individual_channel_battery_energy_rewards = []\n",
    "individual_delay_rewards = []\n",
    "individual_queue_delays = []\n",
    "individual_tasks_dropped = []\n",
    "individual_energy_efficiency = []\n",
    "individual_total_reward = []\n",
    "total_reward = []\n",
    "overall_users_reward = []\n",
    "q_actions = []\n",
    "RB_bandwidths = []\n",
    "rate_variances = []\n",
    "inf_task_delays = []\n",
    "urllc_avg_rate = []\n",
    "individual_channel_rates = []\n",
    "\n",
    "individual_local_queue_delays = []\n",
    "individual_offload_queue_delays = []\n",
    "individual_local_queue_lengths = []\n",
    "individual_offload_queue_lengths = []\n",
    "\n",
    "users_lc_service_rates = []\n",
    "resource_block_action_matrix = []\n",
    "\n",
    "individual_expected_rate_over_prev_T_slot = []\n",
    "individual_average_task_size_offload_queue = []\n",
    "\n",
    "individual_battery_energy_levels = []\n",
    "individual_energy_harvested = []\n",
    "throughput_log_reward = []\n",
    "\n",
    "individual_local_energy_consumed = []\n",
    "individual_offloading_energy = []\n",
    "\n",
    "individual_small_scale_gains = []\n",
    "individual_large_scale_gains = []\n",
    "individual_average_offloading_rates = []\n",
    "\n",
    "\n",
    "individual_local_queue_length_num_tasks = []\n",
    "individual_offload_queue_length_num_tasks = []\n",
    "individual_offload_stability_constraint_reward = []\n",
    "total_offload_traffic_reward = []\n",
    "individual_offload_traffic_numerator = []\n",
    "individual_local_queueing_violation_prob_reward = []\n",
    "individual_offload_ratio_reward = []\n",
    "total_local_queueing_violation_prob_reward = []\n",
    "total_offload_ratio_reward = []\n",
    "urllc_total_rate = []\n",
    "F_L_inverse = []\n",
    "\n",
    "\n",
    "urllc_total_rate_per_second = []\n",
    "urllc_total_rate_per_slot = []\n",
    "individual_urllc_channel_rate_per_slot_with_penalty = []\n",
    "individual_urllc_channel_rate_per_second_penalties = []\n",
    "individual_urllc_channel_rate_per_second_without_penalty = []\n",
    "individual_urllc_channel_rate_per_second_with_penalty = []\n",
    "individual_embb_puncturing_users_sum_data_rates = []\n",
    "individual_embb_num_puncturing_users = []\n",
    "failed_urllc_transmissions = []\n",
    "sum_qeueing_latencies = []\n",
    "\n",
    "individual_simulation_total_delay = []\n",
    "individual_simulation_offload_queueing_delay = []\n",
    "individual_simulation_local_queueing_delay = []\n",
    "total_local_traffic_reward = []\n",
    "total_offload_queueing_violation_prob_reward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training, generate random observation samples to get their limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_step_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "timesteps = np.arange(0,env.STEP_LIMIT,1)\n",
    "num_episodes = 2\n",
    "episodes = np.arange(0,num_episodes,1)\n",
    "obs = env.reset()\n",
    "number_of_users = env.number_of_users\n",
    "number_of_RBs = env.num_allocate_RB_upper_bound\n",
    "small_scale_channel_gains = []\n",
    "large_scale_channel_gains = []\n",
    "battery_energy_levels = []\n",
    "local_queue_lengths = []\n",
    "offloading_queue_lengths = []\n",
    "#observation = np.column_stack((observation_channel_gains,observation_battery_energies,observation_offloading_queue_lengths,observation_local_queue_lengths,num_urllc_arriving_packets)) #observation_channel_gains.\n",
    "for episode in episodes:\n",
    "    for timestep in timesteps:\n",
    "        #print('----------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        action = env.action_space.sample()\n",
    "        action = env.enforce_constraint(action)\n",
    "        #print(action)\n",
    "        action2, action = env.reshape_action_space_dict(action)\n",
    "        observation,reward,dones,info = env.step_(action)\n",
    "        small_scale_channel_gains.append(observation[0:number_of_users*number_of_RBs])\n",
    "        large_scale_channel_gains.append(observation[number_of_users*number_of_RBs:(number_of_users*number_of_RBs)*2])\n",
    "        battery_energy_levels.append(observation[(number_of_users*number_of_RBs)*2:(number_of_users*number_of_RBs)*2+number_of_users])\n",
    "        offloading_queue_lengths.append(observation[(number_of_users*number_of_RBs)*2+number_of_users:(number_of_users*number_of_RBs)*2+number_of_users*2])\n",
    "        local_queue_lengths.append(observation[(number_of_users*number_of_RBs)*2+number_of_users*2:(number_of_users*number_of_RBs)*2+number_of_users*3])\n",
    "        #print('observation:')\n",
    "        #print(observation)\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('small_scale_channel_gains:')\n",
    "        #print(observation[0:number_of_users*number_of_RBs])\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('battery_energy_levels')\n",
    "        #print(observation[(number_of_users*number_of_RBs)*2:(number_of_users*number_of_RBs)*2+number_of_users])\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('large_scale_channel_gains:')\n",
    "        #print(observation[number_of_users*number_of_RBs:(number_of_users*number_of_RBs)*2])\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('offloading_queue_lengths:')\n",
    "        #print(observation[(number_of_users*number_of_RBs)*2+number_of_users:(number_of_users*number_of_RBs)*2+number_of_users*2])\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('local_queue_lengths')\n",
    "        #print(observation[(number_of_users*number_of_RBs)*2+number_of_users*2:(number_of_users*number_of_RBs)*2+number_of_users*3])\n",
    "\n",
    "small_scale_channel_gains = np.array(small_scale_channel_gains)\n",
    "small_scale_channel_gains_x_dim = len(small_scale_channel_gains)\n",
    "small_scale_channel_gains_y_dim = len(small_scale_channel_gains[0])\n",
    "small_scale_channel_gains = small_scale_channel_gains.reshape(1,small_scale_channel_gains_x_dim*small_scale_channel_gains_y_dim)\n",
    "small_scale_channel_gains = small_scale_channel_gains.squeeze()\n",
    "\n",
    "large_scale_channel_gains = np.array(large_scale_channel_gains)\n",
    "large_scale_channel_gains_x_dim = len(large_scale_channel_gains)\n",
    "large_scale_channel_gains_y_dim = len(large_scale_channel_gains[0])\n",
    "large_scale_channel_gains = large_scale_channel_gains.reshape(1,large_scale_channel_gains_x_dim*large_scale_channel_gains_y_dim)\n",
    "large_scale_channel_gains = large_scale_channel_gains.squeeze()\n",
    "\n",
    "offloading_queue_lengths = np.array(offloading_queue_lengths)\n",
    "offloading_queue_lengths_x_dim = len(offloading_queue_lengths)\n",
    "offloading_queue_lengths_y_dim = len(offloading_queue_lengths[0])\n",
    "offloading_queue_lengths = offloading_queue_lengths.reshape(1,offloading_queue_lengths_x_dim*offloading_queue_lengths_y_dim)\n",
    "offloading_queue_lengths = offloading_queue_lengths.squeeze()\n",
    "\n",
    "local_queue_lengths = np.array(local_queue_lengths)\n",
    "local_queue_lengths_x_dim = len(local_queue_lengths)\n",
    "local_queue_lengths_y_dim = len(local_queue_lengths[0])\n",
    "local_queue_lengths = local_queue_lengths.reshape(1,local_queue_lengths_x_dim*local_queue_lengths_y_dim)\n",
    "local_queue_lengths = local_queue_lengths.squeeze()\n",
    "\n",
    "battery_energy_levels = np.array(battery_energy_levels)\n",
    "battery_energy_levels_x_dim = len(battery_energy_levels)\n",
    "battery_energy_levels_y_dim = len(battery_energy_levels[0])\n",
    "battery_energy_levels = battery_energy_levels.reshape(1,battery_energy_levels_x_dim*battery_energy_levels_y_dim)\n",
    "battery_energy_levels = battery_energy_levels.squeeze()\n",
    "\n",
    "# print('small_scale_channel_gains:')\n",
    "# print(small_scale_channel_gains)\n",
    "# print('----------------------------------------------------------')\n",
    "# print('large_scale_channel_gains:')\n",
    "# print(large_scale_channel_gains)\n",
    "# print('----------------------------------------------------------')\n",
    "# print('offloading_queue_lengths:')\n",
    "# print(offloading_queue_lengths)\n",
    "# print('----------------------------------------------------------')\n",
    "# print('local_queue_lengths')\n",
    "# print(local_queue_lengths)\n",
    "# print('----------------------------------------------------------')\n",
    "# print('battery_energy_levels')\n",
    "# print(battery_energy_levels)\n",
    "\n",
    "max_small_scale_channel_gain = max(small_scale_channel_gains)\n",
    "min_small_scale_channel_gain = min(small_scale_channel_gains)\n",
    "\n",
    "max_large_scale_channel_gain = max(large_scale_channel_gains)\n",
    "min_large_scale_channel_gain = min(large_scale_channel_gains)\n",
    "\n",
    "min_local_queue_length = 0\n",
    "min_offloading_queue_length = 0\n",
    "\n",
    "max_local_queue_length = max(local_queue_lengths)\n",
    "max_offloading_queue_length = max(offloading_queue_lengths)\n",
    "\n",
    "max_local_queue_length_ = max(max_local_queue_length,max_offloading_queue_length)\n",
    "max_offloading_queue_length_ = max(max_local_queue_length,max_offloading_queue_length)\n",
    "\n",
    "min_battery_energy_level = 0\n",
    "max_battery_energy_level = max(battery_energy_levels)\n",
    "\n",
    "env.max_small_scale_channel_gain = max_small_scale_channel_gain\n",
    "env.min_small_scale_channel_gain = min_small_scale_channel_gain\n",
    "\n",
    "env.max_battery_energy_level = max_battery_energy_level\n",
    "env.min_battery_energy_level = min_battery_energy_level\n",
    "\n",
    "env.max_large_scale_channel_gain = max_large_scale_channel_gain\n",
    "env.min_large_scale_channel_gain = min_large_scale_channel_gain\n",
    "\n",
    "env.max_local_queue_length = max_local_queue_length_\n",
    "env.min_local_queue_length = min_local_queue_length\n",
    "\n",
    "env.max_offloading_queue_length = max_offloading_queue_length_\n",
    "env.min_offloading_queue_length = min_offloading_queue_length\n",
    "\n",
    "env.change_state_limits(min_small_scale_channel_gain,max_small_scale_channel_gain,\n",
    "                            min_large_scale_channel_gain,max_large_scale_channel_gain,\n",
    "                            min_battery_energy_level,max_battery_energy_level,\n",
    "                            min_local_queue_length,max_local_queue_length_,\n",
    "                            min_offloading_queue_length,max_offloading_queue_length_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_step_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_ouY4NH_Y0I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 101 Episode Num: 1 Reward: 1248908307.8850725\n",
      "Total Timesteps: 202 Episode Num: 2 Reward: 713738105.1565124\n",
      "Total Timesteps: 303 Episode Num: 3 Reward: 770932789.9233127\n",
      "Total Timesteps: 404 Episode Num: 4 Reward: 961083228.3797016\n",
      "Total Timesteps: 505 Episode Num: 5 Reward: 1141087736.6061523\n",
      "Total Timesteps: 606 Episode Num: 6 Reward: 994811410.2047467\n",
      "Total Timesteps: 707 Episode Num: 7 Reward: 1064792332.4738948\n",
      "Total Timesteps: 808 Episode Num: 8 Reward: 847253527.7135117\n",
      "Total Timesteps: 909 Episode Num: 9 Reward: 1225374372.9862688\n",
      "Total Timesteps: 1010 Episode Num: 10 Reward: 1078950678.6632295\n",
      "Total Timesteps: 1111 Episode Num: 11 Reward: 1275417892.2957556\n",
      "Total Timesteps: 1212 Episode Num: 12 Reward: 1380034896.2588832\n",
      "Total Timesteps: 1313 Episode Num: 13 Reward: 1153492047.0003023\n",
      "Total Timesteps: 1414 Episode Num: 14 Reward: 1290861480.045886\n",
      "Total Timesteps: 1515 Episode Num: 15 Reward: 814640696.7892092\n",
      "Total Timesteps: 1616 Episode Num: 16 Reward: 1307652123.1161785\n",
      "Total Timesteps: 1717 Episode Num: 17 Reward: 1361304509.1638253\n",
      "Total Timesteps: 1818 Episode Num: 18 Reward: 944839908.4244034\n",
      "Total Timesteps: 1919 Episode Num: 19 Reward: 890256894.5665777\n",
      "Total Timesteps: 2020 Episode Num: 20 Reward: 1021314303.4287795\n",
      "Total Timesteps: 2121 Episode Num: 21 Reward: 1179557791.3298123\n",
      "Total Timesteps: 2222 Episode Num: 22 Reward: 677018501.1489838\n",
      "Total Timesteps: 2323 Episode Num: 23 Reward: 1378130374.953555\n",
      "Total Timesteps: 2424 Episode Num: 24 Reward: 722240474.9791349\n",
      "Total Timesteps: 2525 Episode Num: 25 Reward: 1274737435.2508132\n",
      "Total Timesteps: 2626 Episode Num: 26 Reward: 1201459159.8332598\n",
      "Total Timesteps: 2727 Episode Num: 27 Reward: 1470227714.9655225\n",
      "Total Timesteps: 2828 Episode Num: 28 Reward: 1103776073.3915281\n",
      "Total Timesteps: 2929 Episode Num: 29 Reward: 1166001219.626861\n",
      "Total Timesteps: 3030 Episode Num: 30 Reward: 1325774411.3351681\n",
      "Total Timesteps: 3131 Episode Num: 31 Reward: 1131466183.898363\n",
      "Total Timesteps: 3232 Episode Num: 32 Reward: 992518595.9013709\n",
      "Total Timesteps: 3333 Episode Num: 33 Reward: 1214937431.2576365\n",
      "Total Timesteps: 3434 Episode Num: 34 Reward: 1109135612.5928583\n",
      "Total Timesteps: 3535 Episode Num: 35 Reward: 1386110734.0961876\n",
      "Total Timesteps: 3636 Episode Num: 36 Reward: 954367305.780552\n",
      "Total Timesteps: 3737 Episode Num: 37 Reward: 1265197663.813315\n",
      "Total Timesteps: 3838 Episode Num: 38 Reward: 1173497125.3502285\n",
      "Total Timesteps: 3939 Episode Num: 39 Reward: 1102162623.188905\n",
      "Total Timesteps: 4040 Episode Num: 40 Reward: 1180149288.6859002\n",
      "Total Timesteps: 4141 Episode Num: 41 Reward: 1185286251.5669787\n",
      "Total Timesteps: 4242 Episode Num: 42 Reward: 598645317.8377202\n",
      "Total Timesteps: 4343 Episode Num: 43 Reward: 1613636923.1293957\n",
      "Total Timesteps: 4444 Episode Num: 44 Reward: 1002848588.4912089\n",
      "Total Timesteps: 4545 Episode Num: 45 Reward: 1256414985.399727\n",
      "Total Timesteps: 4646 Episode Num: 46 Reward: 1136055881.1303718\n",
      "Total Timesteps: 4747 Episode Num: 47 Reward: 1752823232.5815148\n",
      "Total Timesteps: 4848 Episode Num: 48 Reward: 687633513.9174572\n",
      "Total Timesteps: 4949 Episode Num: 49 Reward: 1137405245.4599051\n",
      "Total Timesteps: 5050 Episode Num: 50 Reward: 1254137545.7164392\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13952820141.549284\n",
      "---------------------------------------\n",
      "Total Timesteps: 5151 Episode Num: 51 Reward: 987343932.3721036\n",
      "Total Timesteps: 5252 Episode Num: 52 Reward: 794198584.5189127\n",
      "Total Timesteps: 5353 Episode Num: 53 Reward: 1271468124.8220448\n",
      "Total Timesteps: 5454 Episode Num: 54 Reward: 1122463246.3822231\n",
      "Total Timesteps: 5555 Episode Num: 55 Reward: 1498243723.9055738\n",
      "Total Timesteps: 5656 Episode Num: 56 Reward: 1106227494.9998178\n",
      "Total Timesteps: 5757 Episode Num: 57 Reward: 1514481924.9014268\n",
      "Total Timesteps: 5858 Episode Num: 58 Reward: 1064040033.8627338\n",
      "Total Timesteps: 5959 Episode Num: 59 Reward: 565182036.3276039\n",
      "Total Timesteps: 6060 Episode Num: 60 Reward: 899081105.1154885\n",
      "Total Timesteps: 6161 Episode Num: 61 Reward: 1228790243.4045558\n",
      "Total Timesteps: 6262 Episode Num: 62 Reward: 1232490424.0031002\n",
      "Total Timesteps: 6363 Episode Num: 63 Reward: 611202029.8998364\n",
      "Total Timesteps: 6464 Episode Num: 64 Reward: 1049921943.7381996\n",
      "Total Timesteps: 6565 Episode Num: 65 Reward: 966668564.4109654\n",
      "Total Timesteps: 6666 Episode Num: 66 Reward: 1120593232.6574974\n",
      "Total Timesteps: 6767 Episode Num: 67 Reward: 1100835212.839683\n",
      "Total Timesteps: 6868 Episode Num: 68 Reward: 1631100640.0485575\n",
      "Total Timesteps: 6969 Episode Num: 69 Reward: 1013122958.8454945\n",
      "Total Timesteps: 7070 Episode Num: 70 Reward: 753947697.1249423\n",
      "Total Timesteps: 7171 Episode Num: 71 Reward: 764571901.1625922\n",
      "Total Timesteps: 7272 Episode Num: 72 Reward: 576449010.2291882\n",
      "Total Timesteps: 7373 Episode Num: 73 Reward: 1290600284.0939806\n",
      "Total Timesteps: 7474 Episode Num: 74 Reward: 1264478407.6866314\n",
      "Total Timesteps: 7575 Episode Num: 75 Reward: 857109713.4662924\n",
      "Total Timesteps: 7676 Episode Num: 76 Reward: 1251177179.0390205\n",
      "Total Timesteps: 7777 Episode Num: 77 Reward: 1299160300.8545644\n",
      "Total Timesteps: 7878 Episode Num: 78 Reward: 747052131.9696139\n",
      "Total Timesteps: 7979 Episode Num: 79 Reward: 943333645.5673013\n",
      "Total Timesteps: 8080 Episode Num: 80 Reward: 855198871.6004308\n",
      "Total Timesteps: 8181 Episode Num: 81 Reward: 526997835.62742025\n",
      "Total Timesteps: 8282 Episode Num: 82 Reward: 1211692850.4940715\n",
      "Total Timesteps: 8383 Episode Num: 83 Reward: 857011525.6260073\n",
      "Total Timesteps: 8484 Episode Num: 84 Reward: 997263279.9302808\n",
      "Total Timesteps: 8585 Episode Num: 85 Reward: 1168929329.1035953\n",
      "Total Timesteps: 8686 Episode Num: 86 Reward: 1149329647.7852774\n",
      "Total Timesteps: 8787 Episode Num: 87 Reward: 1643529895.5976973\n",
      "Total Timesteps: 8888 Episode Num: 88 Reward: 1088054915.2293384\n",
      "Total Timesteps: 8989 Episode Num: 89 Reward: 1220300358.904529\n",
      "Total Timesteps: 9090 Episode Num: 90 Reward: 1145261165.7749507\n",
      "Total Timesteps: 9191 Episode Num: 91 Reward: 1071424485.4936982\n",
      "Total Timesteps: 9292 Episode Num: 92 Reward: 914121987.4609165\n",
      "Total Timesteps: 9393 Episode Num: 93 Reward: 1380101535.4220703\n",
      "Total Timesteps: 9494 Episode Num: 94 Reward: 979999911.6395125\n",
      "Total Timesteps: 9595 Episode Num: 95 Reward: 1288580877.9828434\n",
      "Total Timesteps: 9696 Episode Num: 96 Reward: 923702683.9643873\n",
      "Total Timesteps: 9797 Episode Num: 97 Reward: 1185544320.5979843\n",
      "Total Timesteps: 9898 Episode Num: 98 Reward: 1025251434.2360173\n",
      "Total Timesteps: 9999 Episode Num: 99 Reward: 1111116328.7472014\n",
      "Total Timesteps: 10100 Episode Num: 100 Reward: 9745430884.116066\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 14333708971.726009\n",
      "---------------------------------------\n",
      "Total Timesteps: 10201 Episode Num: 101 Reward: 9723127361.29814\n",
      "Total Timesteps: 10302 Episode Num: 102 Reward: 9843175136.978323\n",
      "Total Timesteps: 10403 Episode Num: 103 Reward: 9726811385.58986\n",
      "Total Timesteps: 10504 Episode Num: 104 Reward: 9786715768.52112\n",
      "Total Timesteps: 10605 Episode Num: 105 Reward: 9728578996.284496\n",
      "Total Timesteps: 10706 Episode Num: 106 Reward: 9840388887.663635\n",
      "Total Timesteps: 10807 Episode Num: 107 Reward: 9731557289.64578\n",
      "Total Timesteps: 10908 Episode Num: 108 Reward: 9795902083.355326\n",
      "Total Timesteps: 11009 Episode Num: 109 Reward: 9813890513.441418\n",
      "Total Timesteps: 11110 Episode Num: 110 Reward: 9907177963.748976\n",
      "Total Timesteps: 11211 Episode Num: 111 Reward: 10049755203.209808\n",
      "Total Timesteps: 11312 Episode Num: 112 Reward: 9933966888.42156\n",
      "Total Timesteps: 11413 Episode Num: 113 Reward: 9528252268.69257\n",
      "Total Timesteps: 11514 Episode Num: 114 Reward: 9963555077.574846\n",
      "Total Timesteps: 11615 Episode Num: 115 Reward: 10105280364.876183\n",
      "Total Timesteps: 11716 Episode Num: 116 Reward: 9924448009.574059\n",
      "Total Timesteps: 11817 Episode Num: 117 Reward: 9972453963.447536\n",
      "Total Timesteps: 11918 Episode Num: 118 Reward: 10091048806.617037\n",
      "Total Timesteps: 12019 Episode Num: 119 Reward: 9916890290.453846\n",
      "Total Timesteps: 12120 Episode Num: 120 Reward: 10123196254.687956\n",
      "Total Timesteps: 12221 Episode Num: 121 Reward: 9827145480.331097\n",
      "Total Timesteps: 12322 Episode Num: 122 Reward: 9679512754.953255\n",
      "Total Timesteps: 12423 Episode Num: 123 Reward: 10051420859.20957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 12524 Episode Num: 124 Reward: 10229026189.672745\n",
      "Total Timesteps: 12625 Episode Num: 125 Reward: 9817653101.24718\n",
      "Total Timesteps: 12726 Episode Num: 126 Reward: 9670117465.379768\n",
      "Total Timesteps: 12827 Episode Num: 127 Reward: 10123069167.33448\n",
      "Total Timesteps: 12928 Episode Num: 128 Reward: 9355064871.963882\n",
      "Total Timesteps: 13029 Episode Num: 129 Reward: 10165943176.782856\n",
      "Total Timesteps: 13130 Episode Num: 130 Reward: 9864466394.736132\n",
      "Total Timesteps: 13231 Episode Num: 131 Reward: 9601247238.649092\n",
      "Total Timesteps: 13332 Episode Num: 132 Reward: 9608427696.928534\n",
      "Total Timesteps: 13433 Episode Num: 133 Reward: 9634669198.921389\n",
      "Total Timesteps: 13534 Episode Num: 134 Reward: 9725668598.481451\n",
      "Total Timesteps: 13635 Episode Num: 135 Reward: 9538478520.592916\n",
      "Total Timesteps: 13736 Episode Num: 136 Reward: 9981082861.559633\n",
      "Total Timesteps: 13837 Episode Num: 137 Reward: 9340811113.683033\n",
      "Total Timesteps: 13938 Episode Num: 138 Reward: 9648053723.27583\n",
      "Total Timesteps: 14039 Episode Num: 139 Reward: 9746685365.60431\n",
      "Total Timesteps: 14140 Episode Num: 140 Reward: 9856663136.239632\n",
      "Total Timesteps: 14241 Episode Num: 141 Reward: 9964891104.634047\n",
      "Total Timesteps: 14342 Episode Num: 142 Reward: 10073986931.788004\n",
      "Total Timesteps: 14443 Episode Num: 143 Reward: 9898944234.607635\n",
      "Total Timesteps: 14544 Episode Num: 144 Reward: 9785763542.28503\n",
      "Total Timesteps: 14645 Episode Num: 145 Reward: 9712525225.239323\n",
      "Total Timesteps: 14746 Episode Num: 146 Reward: 10013190524.18096\n",
      "Total Timesteps: 14847 Episode Num: 147 Reward: 9989049423.148643\n",
      "Total Timesteps: 14948 Episode Num: 148 Reward: 9945510145.443594\n",
      "Total Timesteps: 15049 Episode Num: 149 Reward: 10257664370.853846\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 14260455177.633017\n",
      "---------------------------------------\n",
      "Total Timesteps: 15150 Episode Num: 150 Reward: 9839861116.750769\n",
      "Total Timesteps: 15251 Episode Num: 151 Reward: 9867794280.681433\n",
      "Total Timesteps: 15352 Episode Num: 152 Reward: 9838052652.811165\n",
      "Total Timesteps: 15453 Episode Num: 153 Reward: 9323632337.771305\n",
      "Total Timesteps: 15554 Episode Num: 154 Reward: 9567870993.127031\n",
      "Total Timesteps: 15655 Episode Num: 155 Reward: 9953558364.445429\n",
      "Total Timesteps: 15756 Episode Num: 156 Reward: 10002142702.692736\n",
      "Total Timesteps: 15857 Episode Num: 157 Reward: 9631933043.235022\n",
      "Total Timesteps: 15958 Episode Num: 158 Reward: 10260827362.195145\n",
      "Total Timesteps: 16059 Episode Num: 159 Reward: 10217456917.358047\n",
      "Total Timesteps: 16160 Episode Num: 160 Reward: 10076432039.025373\n",
      "Total Timesteps: 16261 Episode Num: 161 Reward: 10045077067.451298\n",
      "Total Timesteps: 16362 Episode Num: 162 Reward: 9972195086.736122\n",
      "Total Timesteps: 16463 Episode Num: 163 Reward: 9662100139.389267\n",
      "Total Timesteps: 16564 Episode Num: 164 Reward: 9690562293.429918\n",
      "Total Timesteps: 16665 Episode Num: 165 Reward: 10073518295.124165\n",
      "Total Timesteps: 16766 Episode Num: 166 Reward: 9738757751.33163\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      timestep_rewards.append([total_timesteps, episode_reward])\n",
    "      timestep_rewards_energy_throughput_delays.append([total_timesteps,episode_reward,env.total_energy,env.total_rate,env.SBS1.total_delay])\n",
    "      offload_actions.append(env.offload_decisions)\n",
    "      power_actions.append(env.powers)\n",
    "      subcarrier_actions.append(env.subcarriers)\n",
    "      allocated_RBs.append(env.Communication_Channel_1.allocated_RBs)\n",
    "      fairness_index.append(env.SBS1.fairness_index)\n",
    "      outage_probabilties.append(env.SBS1.outage_probability)\n",
    "\n",
    "      energy_efficiency_rewards.append(env.SBS1.energy_efficiency_rewards)\n",
    "      battery_energy_rewards.append(env.SBS1.battery_energy_rewards)\n",
    "      throughput_rewards.append(env.SBS1.throughput_rewards)\n",
    "      delay_rewards.append(env.SBS1.delay_rewards)\n",
    "      sum_allocations_per_RB_matrix.append(env.sum_allocations_per_RB_matrix)\n",
    "      RB_allocation_matrix.append(env.RB_allocation_matrix)\n",
    "      energy_rewards.append(env.SBS1.energy_rewards)\n",
    "      delays.append(env.SBS1.delays)\n",
    "      tasks_dropped.append(env.SBS1.tasks_dropped)\n",
    "      resource_allocation_matrix.append(env.resource_block_allocation_matrix)\n",
    "      resource_allocation_constraint_violation_count.append(env.resource_allocation_constraint_violation)\n",
    "      urllc_reliability_reward.append(env.SBS1.urllc_reliability_reward)\n",
    "\n",
    "      individual_energy_rewards.append(env.SBS1.individual_energy_rewards)\n",
    "      individual_channel_rate_rewards.append(env.SBS1.individual_channel_rate_rewards)\n",
    "      individual_channel_rates.append(env.SBS1.individual_channel_rates)\n",
    "      individual_channel_battery_energy_rewards.append(env.SBS1.individual_channel_rate_rewards)\n",
    "      individual_delay_rewards.append(env.SBS1.individual_delay_rewards)\n",
    "      individual_queue_delays.append(env.SBS1.individual_queue_delays)\n",
    "      individual_tasks_dropped.append(env.SBS1.individual_tasks_dropped)\n",
    "      individual_energy_efficiency.append(env.SBS1.individual_energy_efficiency)\n",
    "      individual_total_reward.append(env.SBS1.individual_total_reward)\n",
    "      total_reward.append(env.SBS1.total_reward)\n",
    "      overall_users_reward.append(env.SBS1.overall_users_reward)\n",
    "      q_actions.append(env.SBS1.q_action)\n",
    "      RB_bandwidths.append(env.RB_bandwidth)\n",
    "      rate_variances.append(env.SBS1.users_rate_variance_sum)\n",
    "      urllc_avg_rate.append(env.SBS1.average_rate_prev_slots)\n",
    "      individual_local_queue_delays.append(env.SBS1.individual_local_queue_delays)\n",
    "      individual_offload_queue_delays.append(env.SBS1.individual_offload_queue_delays)\n",
    "      individual_local_queue_lengths.append(env.SBS1.individual_local_queue_lengths)\n",
    "      individual_offload_queue_lengths.append(env.SBS1.individual_offload_queue_lengths)\n",
    "      users_lc_service_rates.clear()\n",
    "      users_lc_service_rates.append(env.SBS1.users_lc_service_rates)\n",
    "      resource_block_action_matrix.append(env.resource_block_action_matrix)\n",
    "      sum_qeueing_latencies.append(env.SBS1.sum_queueing_latency)\n",
    "\n",
    "      individual_expected_rate_over_prev_T_slot.append(env.SBS1.individual_expected_rate_over_prev_T_slot)\n",
    "      individual_average_task_size_offload_queue.append(env.SBS1.individual_average_task_size_offload_queue)\n",
    "\n",
    "      individual_battery_energy_levels.append(env.SBS1.individual_battery_energy_levels)\n",
    "      individual_energy_harvested.append(env.SBS1.individual_energy_harvested)\n",
    "      throughput_log_reward.append(env.SBS1.throughput_log_reward)\n",
    "      individual_local_energy_consumed.append(env.SBS1.individual_local_energy_consumed)\n",
    "      individual_offloading_energy.append(env.SBS1.individual_offloading_energy)\n",
    "      individual_small_scale_gains.append(env.SBS1.individual_small_scale_gains)\n",
    "      individual_large_scale_gains.append(env.SBS1.individual_large_scale_gains)\n",
    "      individual_average_offloading_rates.append(env.SBS1.individual_average_offloading_rates)\n",
    "      individual_local_queue_length_num_tasks.append(env.SBS1.individual_local_queue_length_num_tasks)\n",
    "      individual_offload_queue_length_num_tasks.append(env.SBS1.individual_offload_queue_length_num_tasks)\n",
    "      individual_offload_stability_constraint_reward.append(env.SBS1.individual_offload_stability_constraint_reward)\n",
    "      total_offload_traffic_reward.append(env.SBS1.total_offload_traffic_reward)\n",
    "      individual_offload_traffic_numerator.append(env.SBS1.individual_offload_traffic_numerator)\n",
    "      individual_local_queueing_violation_prob_reward.append(env.SBS1.individual_local_queueing_violation_prob_reward)\n",
    "      individual_offload_ratio_reward.append(env.SBS1.individual_offload_ratio_reward)\n",
    "      total_local_queueing_violation_prob_reward.append(env.SBS1.total_local_queueing_violation_prob_reward)\n",
    "      total_local_traffic_reward.append(env.SBS1.total_local_traffic_reward)\n",
    "      total_offload_ratio_reward.append(env.SBS1.total_offload_ratio_reward)\n",
    "      urllc_total_rate.append(env.SBS1.urllc_total_rate)\n",
    "      F_L_inverse.append(env.SBS1.F_L_inverse)\n",
    "      failed_urllc_transmissions.append(env.SBS1.failed_urllc_transmissions)\n",
    "\n",
    "      urllc_total_rate_per_second.append(env.SBS1.urllc_total_rate_per_second)\n",
    "      urllc_total_rate_per_slot.append(env.SBS1.urllc_total_rate_per_slot)\n",
    "      individual_urllc_channel_rate_per_slot_with_penalty.append(env.SBS1.individual_urllc_channel_rate_per_slot_with_penalty)\n",
    "      individual_urllc_channel_rate_per_second_penalties.append(env.SBS1.individual_urllc_channel_rate_per_second_penalties)\n",
    "      individual_urllc_channel_rate_per_second_without_penalty.append(env.SBS1.individual_urllc_channel_rate_per_second_without_penalty)\n",
    "      individual_urllc_channel_rate_per_second_with_penalty.append(env.SBS1.individual_urllc_channel_rate_per_second_with_penalty)\n",
    "      individual_embb_puncturing_users_sum_data_rates.append(env.SBS1.individual_embb_puncturing_users_sum_data_rates)\n",
    "      individual_embb_num_puncturing_users.append(env.SBS1.individual_embb_num_puncturing_users)\n",
    "      individual_simulation_total_delay.append(env.SBS1.individual_simulation_total_delay)\n",
    "      individual_simulation_offload_queueing_delay.append(env.SBS1.individual_simulation_offload_queueing_delay)\n",
    "      individual_simulation_local_queueing_delay.append(env.SBS1.individual_simulation_local_queueing_delay)\n",
    "      total_offload_queueing_violation_prob_reward.append(env.SBS1.total_offload_queueing_violation_prob_reward)\n",
    "\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "      np.save(\"./results/%s\" % (file_name_1), timestep_rewards_energy_throughput_delays)\n",
    "      np.save(\"./results/%s\" % (file_name_2), offload_actions)\n",
    "      np.save(\"./results/%s\" % (file_name_3), power_actions)\n",
    "      np.save(\"./results/%s\" % (file_name_4), subcarrier_actions)\n",
    "      np.save(\"./results/%s\" % (file_name_5), allocated_RBs)\n",
    "      np.save(\"./results/%s\" % (file_name_6), fairness_index)\n",
    "      np.save(\"./results/%s\" % (file_name_7), energy_efficiency_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_8), battery_energy_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_9), throughput_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_10), delay_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_11), sum_allocations_per_RB_matrix)\n",
    "      np.save(\"./results/%s\" % (file_name_12), RB_allocation_matrix)\n",
    "      np.save(\"./results/%s\" % (file_name_13), energy_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_14), delays)\n",
    "      np.save(\"./results/%s\" % (file_name_15), tasks_dropped)\n",
    "      np.save(\"./results/%s\" % (file_name_16), outage_probabilties)\n",
    "      np.save(\"./results/%s\" % (file_name_17), resource_allocation_constraint_violation_count)\n",
    "      np.save(\"./results/%s\" % (file_name_18), urllc_reliability_reward)\n",
    "\n",
    "      np.save(\"./results/%s\" % (file_name_19), individual_energy_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_20), individual_channel_rate_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_21), individual_channel_battery_energy_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_22), individual_delay_rewards)\n",
    "      np.save(\"./results/%s\" % (file_name_23), individual_queue_delays)\n",
    "      np.save(\"./results/%s\" % (file_name_24), individual_tasks_dropped)\n",
    "      np.save(\"./results/%s\" % (file_name_25), individual_energy_efficiency)\n",
    "      np.save(\"./results/%s\" % (file_name_26), individual_total_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_27), total_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_28), overall_users_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_29), q_actions)\n",
    "      np.save(\"./results/%s\" % (file_name_30), RB_bandwidths)\n",
    "      np.save(\"./results/%s\" % (file_name_37), urllc_avg_rate)\n",
    "      np.save(\"./results/%s\" % (file_name_38), individual_channel_rates)\n",
    "\n",
    "      np.save(\"./results/%s\" % (file_name_39), individual_local_queue_delays)\n",
    "      np.save(\"./results/%s\" % (file_name_40), individual_offload_queue_delays)\n",
    "      np.save(\"./results/%s\" % (file_name_41), individual_local_queue_lengths)\n",
    "      np.save(\"./results/%s\" % (file_name_42), individual_offload_queue_lengths)\n",
    "      np.save(\"./results/%s\" % (file_name_43), users_lc_service_rates)\n",
    "      np.save(\"./results/%s\" % (file_name_45), individual_expected_rate_over_prev_T_slot)\n",
    "      np.save(\"./results/%s\" % (file_name_46), individual_average_task_size_offload_queue)\n",
    "      np.save(\"./results/%s\" % (file_name_47), individual_battery_energy_levels)\n",
    "      np.save(\"./results/%s\" % (file_name_48), individual_energy_harvested)\n",
    "      np.save(\"./results/%s\" % (file_name_49), throughput_log_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_50), individual_local_energy_consumed)\n",
    "      np.save(\"./results/%s\" % (file_name_51), individual_offloading_energy)\n",
    "      np.save(\"./results/%s\" % (file_name_52), individual_small_scale_gains)\n",
    "      np.save(\"./results/%s\" % (file_name_53), individual_large_scale_gains)\n",
    "      np.save(\"./results/%s\" % (file_name_54), individual_average_offloading_rates)\n",
    "      np.save(\"./results/%s\" % (file_name_55), individual_local_queue_length_num_tasks)\n",
    "      np.save(\"./results/%s\" % (file_name_56), individual_offload_queue_length_num_tasks)\n",
    "      np.save(\"./results/%s\" % (file_name_57), individual_offload_stability_constraint_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_58), total_offload_traffic_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_59), individual_offload_traffic_numerator)\n",
    "      np.save(\"./results/%s\" % (file_name_60), individual_local_queueing_violation_prob_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_61), individual_offload_ratio_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_62), total_local_queueing_violation_prob_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_63), total_offload_ratio_reward)\n",
    "      np.save(\"./results/%s\" % (file_name_64), urllc_total_rate)\n",
    "      np.save(\"./results/%s\" % (file_name_65), F_L_inverse)\n",
    "\n",
    "      np.save(\"./results/%s\" % (file_name_66), urllc_total_rate_per_second)\n",
    "      np.save(\"./results/%s\" % (file_name_67), urllc_total_rate_per_slot)\n",
    "      np.save(\"./results/%s\" % (file_name_68), individual_urllc_channel_rate_per_slot_with_penalty)\n",
    "      np.save(\"./results/%s\" % (file_name_69), individual_urllc_channel_rate_per_second_penalties)\n",
    "      np.save(\"./results/%s\" % (file_name_70), individual_urllc_channel_rate_per_second_without_penalty)\n",
    "      np.save(\"./results/%s\" % (file_name_71), individual_urllc_channel_rate_per_second_with_penalty)\n",
    "\n",
    "      np.save(\"./results/%s\" % (file_name_72), individual_embb_puncturing_users_sum_data_rates)\n",
    "      np.save(\"./results/%s\" % (file_name_73), individual_embb_num_puncturing_users)    \n",
    "      np.save(\"./results/%s\" % (file_name_74), failed_urllc_transmissions)   \n",
    "      np.save(\"./results/%s\" % (file_name_75), sum_qeueing_latencies)   \n",
    "      np.save(\"./results/%s\" % (file_name_76), individual_simulation_total_delay)  \n",
    "      np.save(\"./results/%s\" % (file_name_77), individual_simulation_offload_queueing_delay)  \n",
    "      np.save(\"./results/%s\" % (file_name_78), individual_simulation_local_queueing_delay)  \n",
    "      np.save(\"./results/%s\" % (file_name_79), total_local_traffic_reward) \n",
    "      np.save(\"./results/%s\" % (file_name_80), total_offload_queueing_violation_prob_reward) \n",
    "\n",
    "\n",
    "\n",
    "      #np.save(\"./results/%s\" % (file_name_44), resource_block_action_matrix)\n",
    "      \n",
    "      \n",
    "      \n",
    "\n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "    action = env.enforce_constraint(action)\n",
    "    action2, action = env.reshape_action_space_dict(action)\n",
    "    \n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    #print('action before noise applied:')\n",
    "    #print(action)\n",
    "    if expl_noise != 0:\n",
    "      #print('env.action_space.shape: ', env.action_space_dim)\n",
    "      noise = np.random.normal(0, expl_noise, size=env.action_space_dim)\n",
    "      #print('noise:')\n",
    "      #print(noise)\n",
    "      action = (action + noise).clip(env.action_space_low, env.action_space_high)\n",
    "\n",
    "    #print('action after noise applied:')\n",
    "    #print(action)\n",
    "    #print('')\n",
    "    action = env.reshape_action_space_from_model_to_dict(action)\n",
    "    # print('action')\n",
    "    # print(action)\n",
    "    #reformed_action = env.apply_resource_allocation_constraint(action)\n",
    "    mode = 'training'\n",
    "    action = env.apply_resource_allocation_constraint(action,mode)\n",
    "\n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    #action = env.enforce_constraint(action)\n",
    "    #print(action)\n",
    "    \n",
    "  \n",
    "  #print(\"Action in training\")\n",
    "  #print(action)\n",
    "  #print(' ')\n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  #new_obs, reward, done, _ = env.step(action)\n",
    "  #done = dones[len(dones) - 1]\n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env.STEP_LIMIT else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  #print('episode_reward: ', episode_reward)\n",
    "  #print('episode reward')\n",
    "  #print(episode_reward)\n",
    "  #episode_reward = interp(episode_reward,[720000000,863000000],[0,1000])\n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  action = env.reshape_action_space_for_model(action)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "np.save(\"./results/%s\" % (file_name_1), timestep_rewards_energy_throughput_delays)\n",
    "np.save(\"./results/%s\" % (file_name_2), offload_actions)\n",
    "np.save(\"./results/%s\" % (file_name_3), power_actions)\n",
    "np.save(\"./results/%s\" % (file_name_4), subcarrier_actions)\n",
    "np.save(\"./results/%s\" % (file_name_5), allocated_RBs)\n",
    "np.save(\"./results/%s\" % (file_name_6), fairness_index)\n",
    "np.save(\"./results/%s\" % (file_name_7), energy_efficiency_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_8), battery_energy_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_9), throughput_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_10), delay_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_11), sum_allocations_per_RB_matrix)\n",
    "np.save(\"./results/%s\" % (file_name_12), RB_allocation_matrix)\n",
    "np.save(\"./results/%s\" % (file_name_13), energy_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_14), delays)\n",
    "np.save(\"./results/%s\" % (file_name_15), tasks_dropped)\n",
    "np.save(\"./results/%s\" % (file_name_16), outage_probabilties)\n",
    "np.save(\"./results/%s\" % (file_name_17), resource_allocation_constraint_violation_count)\n",
    "np.save(\"./results/%s\" % (file_name_18), urllc_reliability_reward)\n",
    "\n",
    "np.save(\"./results/%s\" % (file_name_19), individual_energy_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_20), individual_channel_rate_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_21), individual_channel_battery_energy_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_22), individual_delay_rewards)\n",
    "np.save(\"./results/%s\" % (file_name_23), individual_queue_delays)\n",
    "np.save(\"./results/%s\" % (file_name_24), individual_tasks_dropped)\n",
    "np.save(\"./results/%s\" % (file_name_25), individual_energy_efficiency)\n",
    "np.save(\"./results/%s\" % (file_name_26), individual_total_reward)\n",
    "np.save(\"./results/%s\" % (file_name_27), total_reward)\n",
    "np.save(\"./results/%s\" % (file_name_28), overall_users_reward)\n",
    "np.save(\"./results/%s\" % (file_name_37), urllc_avg_rate)\n",
    "np.save(\"./results/%s\" % (file_name_38), individual_channel_rates)\n",
    "np.save(\"./results/%s\" % (file_name_39), individual_local_queue_delays)\n",
    "np.save(\"./results/%s\" % (file_name_40), individual_offload_queue_delays)\n",
    "np.save(\"./results/%s\" % (file_name_41), individual_local_queue_lengths)\n",
    "np.save(\"./results/%s\" % (file_name_42), individual_offload_queue_lengths)\n",
    "np.save(\"./results/%s\" % (file_name_43), users_lc_service_rates)\n",
    "np.save(\"./results/%s\" % (file_name_45), individual_expected_rate_over_prev_T_slot)\n",
    "np.save(\"./results/%s\" % (file_name_46), individual_average_task_size_offload_queue)\n",
    "np.save(\"./results/%s\" % (file_name_47), individual_battery_energy_levels)\n",
    "np.save(\"./results/%s\" % (file_name_48), individual_energy_harvested)\n",
    "np.save(\"./results/%s\" % (file_name_49), throughput_log_reward)\n",
    "np.save(\"./results/%s\" % (file_name_50), individual_local_energy_consumed)\n",
    "np.save(\"./results/%s\" % (file_name_51), individual_offloading_energy)\n",
    "np.save(\"./results/%s\" % (file_name_52), individual_small_scale_gains)\n",
    "np.save(\"./results/%s\" % (file_name_53), individual_large_scale_gains)\n",
    "np.save(\"./results/%s\" % (file_name_54), individual_average_offloading_rates)\n",
    "np.save(\"./results/%s\" % (file_name_55), individual_local_queue_length_num_tasks)\n",
    "np.save(\"./results/%s\" % (file_name_56), individual_offload_queue_length_num_tasks)\n",
    "np.save(\"./results/%s\" % (file_name_57), individual_offload_stability_constraint_reward)\n",
    "np.save(\"./results/%s\" % (file_name_58), total_offload_traffic_reward)\n",
    "np.save(\"./results/%s\" % (file_name_59), individual_offload_traffic_numerator)\n",
    "np.save(\"./results/%s\" % (file_name_60), individual_local_queueing_violation_prob_reward)\n",
    "np.save(\"./results/%s\" % (file_name_61), individual_offload_ratio_reward)\n",
    "np.save(\"./results/%s\" % (file_name_62), total_local_queueing_violation_prob_reward)\n",
    "np.save(\"./results/%s\" % (file_name_63), total_offload_ratio_reward)\n",
    "np.save(\"./results/%s\" % (file_name_64), urllc_total_rate)\n",
    "np.save(\"./results/%s\" % (file_name_65), F_L_inverse)\n",
    "np.save(\"./results/%s\" % (file_name_66), urllc_total_rate_per_second)\n",
    "np.save(\"./results/%s\" % (file_name_67), urllc_total_rate_per_slot)\n",
    "np.save(\"./results/%s\" % (file_name_68), individual_urllc_channel_rate_per_slot_with_penalty)\n",
    "np.save(\"./results/%s\" % (file_name_69), individual_urllc_channel_rate_per_second_penalties)\n",
    "np.save(\"./results/%s\" % (file_name_70), individual_urllc_channel_rate_per_second_without_penalty)\n",
    "np.save(\"./results/%s\" % (file_name_71), individual_urllc_channel_rate_per_second_with_penalty)\n",
    "np.save(\"./results/%s\" % (file_name_72), individual_embb_puncturing_users_sum_data_rates)\n",
    "np.save(\"./results/%s\" % (file_name_73), individual_embb_num_puncturing_users) \n",
    "np.save(\"./results/%s\" % (file_name_74), failed_urllc_transmissions)   \n",
    "np.save(\"./results/%s\" % (file_name_75), sum_qeueing_latencies)   \n",
    "np.save(\"./results/%s\" % (file_name_76), individual_simulation_total_delay)  \n",
    "np.save(\"./results/%s\" % (file_name_77), individual_simulation_offload_queueing_delay)  \n",
    "np.save(\"./results/%s\" % (file_name_78), individual_simulation_local_queueing_delay) \n",
    "np.save(\"./results/%s\" % (file_name_79), total_local_traffic_reward)  \n",
    "np.save(\"./results/%s\" % (file_name_80), total_offload_queueing_violation_prob_reward) \n",
    "#np.save(\"./results/%s\" % (file_name_44), resource_block_action_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_total_reward = []\n",
    "inf_energy = []\n",
    "inf_throughput = []\n",
    "inf_fairness_index = []\n",
    "inf_num_RBs_allocated = []\n",
    "inf_outage_probability = []\n",
    "inf_individual_channel_rates = []\n",
    "inf_individual_number_of_allocated_RB = []\n",
    "inf_individual_number_of_puncturing_urllc_users = []\n",
    "inf_individual_num_of_clustered_urllc_users = []\n",
    "inf_failed_urllc_transmissions = []\n",
    "inf_individual_offload_ratios = []\n",
    "inf_individual_local_queue_lengths_bits = []\n",
    "inf_individual_offload_queue_lengths_bits = []\n",
    "inf_individual_local_queue_lengths_tasks = []\n",
    "inf_individual_offload_queue_lengths_tasks = []\n",
    "\n",
    "inf_battery_energy_constraint_violation_count = []\n",
    "inf_local_queueing_traffic_constraint_violation_count = []\n",
    "inf_offload_queueing_traffic_constaint_violation_count = []\n",
    "inf_local_time_delay_violation_prob_constraint_violation_count = []\n",
    "inf_rmin_constraint_violation_count = []\n",
    "\n",
    "inf_individual_energy_harvested_levels = []\n",
    "inf_individual_battery_energy_levels = []\n",
    "inf_individual_local_queue_delay_violation_probability = []\n",
    "inf_individual_offload_queue_delay_violation_probability = []\n",
    "inf_total_local_delay = []\n",
    "inf_total_offload_delay = []\n",
    "inf_total_local_queue_length_tasks = []\n",
    "inf_total_offload_queue_length_tasks = []\n",
    "inf_total_local_queue_length_bits = []\n",
    "inf_total_offload_queue_length_bits = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW4d1YAMqif1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inf_total_reward = []\n",
    "inf_energy = []\n",
    "inf_task_delays = []\n",
    "inf_throughput = []\n",
    "inf_fairness_index = []\n",
    "inf_num_RBs_allocated = []\n",
    "inf_outage_probability = []\n",
    "inf_individual_channel_rates = []\n",
    "inf_individual_number_of_allocated_RB = []\n",
    "inf_individual_number_of_puncturing_urllc_users = []\n",
    "inf_individual_num_of_clustered_urllc_users = []\n",
    "inf_failed_urllc_transmissions = []\n",
    "inf_individual_offload_ratios = []\n",
    "inf_individual_local_queue_lengths_bits = []\n",
    "inf_individual_offload_queue_lengths_bits = []\n",
    "inf_individual_local_queue_lengths_tasks = []\n",
    "inf_individual_offload_queue_lengths_tasks = []\n",
    "\n",
    "inf_battery_energy_constraint_violation_count = []\n",
    "inf_local_queueing_traffic_constraint_violation_count = []\n",
    "inf_offload_queueing_traffic_constaint_violation_count = []\n",
    "inf_local_time_delay_violation_prob_constraint_violation_count = []\n",
    "inf_offload_time_delay_violation_prob_constraint_violation_count = []\n",
    "inf_rmin_constraint_violation_count = []\n",
    "inf_individual_energy_harvested_levels = []\n",
    "inf_individual_battery_energy_levels = []\n",
    "inf_individual_local_queue_delay_violation_probability = []\n",
    "inf_individual_offload_queue_delay_violation_probability = []\n",
    "inf_total_local_delay = []\n",
    "inf_total_offload_delay = []\n",
    "inf_total_local_queue_length_tasks = []\n",
    "inf_total_offload_queue_length_tasks = []\n",
    "inf_total_local_queue_length_bits = []\n",
    "inf_total_offload_queue_length_bits = []\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.sigmoid(self.layer_3(x))\n",
    "    #x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  # Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.00001)\n",
    "    #self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.00000001)\n",
    "    \n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.0001)\n",
    "    #self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.00001)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    #return self.actor(state).cpu().data.numpy().flatten()\n",
    "    return self.actor(state).cpu().data.numpy()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "      #batch_states = np.reshape(batch_states,(batch_states.shape[0]*batch_states.shape[1],batch_states.shape[2]))\n",
    "      #batch_next_states = np.reshape(batch_next_states,(batch_next_states.shape[0]*batch_next_states.shape[1],batch_next_states.shape[2]))\n",
    "      #batch_actions = np.reshape(batch_actions,(batch_actions.shape[0]*batch_actions.shape[1],batch_actions.shape[2]))\n",
    "\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes,number_of_users):\n",
    "  inf_outage_probability=[]\n",
    "  avg_reward = 0\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(obs)\n",
    "      noise = np.random.normal(0, expl_noise, size=env.action_space_dim)\n",
    "      #print('noise:')\n",
    "      #print(noise)\n",
    "      action = (action + noise).clip(env.action_space_low, env.action_space_high)\n",
    "      action = env.reshape_action_space_from_model_to_dict(action)\n",
    "      mode = 'inference'\n",
    "      reformed_action = env.apply_resource_allocation_constraint(action,mode)\n",
    "      obs, reward, done, _ = env.step(reformed_action)\n",
    "      inf_energy.append(env.total_energy)\n",
    "      inf_throughput.append(env.total_rate)\n",
    "      inf_total_reward.append(reward)\n",
    "      inf_fairness_index.append(env.SBS1.fairness_index)\n",
    "      inf_task_delays.append(env.SBS1.total_delay)\n",
    "      inf_num_RBs_allocated.append(env.num_RBs_allocated)\n",
    "      inf_outage_probability.append(env.SBS1.outage_probability)\n",
    "      inf_individual_channel_rates.append(env.SBS1.individual_channel_rate_rewards)\n",
    "      inf_individual_number_of_allocated_RB.append(env.SBS1.individual_num_of_allocated_RBs)\n",
    "      inf_individual_number_of_puncturing_urllc_users.append(env.SBS1.individual_embb_num_puncturing_users)\n",
    "      inf_individual_num_of_clustered_urllc_users.append(env.SBS1.individual_num_of_clustered_urllc_users)\n",
    "      inf_failed_urllc_transmissions.append(env.SBS1.failed_urllc_transmissions)\n",
    "      inf_individual_offload_ratios.append(env.offload_decisions)\n",
    "      inf_individual_local_queue_lengths_bits.append(env.SBS1.individual_local_queue_lengths)\n",
    "      inf_individual_offload_queue_lengths_bits.append(env.SBS1.individual_offload_queue_lengths)\n",
    "      inf_individual_local_queue_lengths_tasks.append(env.SBS1.individual_local_queue_length_num_tasks)\n",
    "      inf_individual_offload_queue_lengths_tasks.append(env.SBS1.individual_offload_queue_length_num_tasks)\n",
    "        \n",
    "      inf_battery_energy_constraint_violation_count.append(env.SBS1.battery_energy_constraint_violation_count)\n",
    "      inf_local_queueing_traffic_constraint_violation_count.append(env.SBS1.local_queueing_traffic_constraint_violation_count)\n",
    "      inf_offload_queueing_traffic_constaint_violation_count.append(env.SBS1.offload_queueing_traffic_constaint_violation_count)\n",
    "      inf_local_time_delay_violation_prob_constraint_violation_count.append(env.SBS1.local_time_delay_violation_prob_constraint_violation_count)\n",
    "      inf_offload_time_delay_violation_prob_constraint_violation_count.append(env.SBS1.offload_time_delay_violation_prob_constraint_violation_count)\n",
    "      inf_rmin_constraint_violation_count.append(env.SBS1.rmin_constraint_violation_count)\n",
    "    \n",
    "      inf_individual_energy_harvested_levels.append(env.SBS1.individual_energy_harvested)\n",
    "      inf_individual_battery_energy_levels.append(env.SBS1.individual_battery_energy_levels)\n",
    "      inf_individual_local_queue_delay_violation_probability.append(env.SBS1.individual_local_queue_delay_violation_probability)\n",
    "      inf_individual_offload_queue_delay_violation_probability.append(env.SBS1.individual_offload_queue_delay_violation_probability)\n",
    "      inf_total_local_delay.append(env.SBS1.total_local_delay)\n",
    "      inf_total_offload_delay.append(env.SBS1.total_offload_delay)\n",
    "      inf_total_local_queue_length_tasks.append(env.SBS1.total_local_queue_length_tasks)\n",
    "      inf_total_offload_queue_length_tasks.append(env.SBS1.total_offload_queue_length_tasks)\n",
    "      inf_total_local_queue_length_bits.append(env.SBS1.total_local_queue_length_bits)\n",
    "      inf_total_offload_queue_length_bits.append(env.SBS1.total_offload_queue_length_bits)\n",
    "\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  #print('len(inf_total_reward): ', len(inf_total_reward))\n",
    "  av_reward = sum(inf_total_reward)/len(inf_total_reward)\n",
    "  av_energy = sum(inf_energy)/len(inf_energy)\n",
    "  #print('inf_throughput: ', inf_throughput)\n",
    "  av_throughput = sum(inf_throughput)/len(inf_throughput)\n",
    "  #print('len(inf_throughput): ', len(inf_throughput))\n",
    "  #print('sum(inf_throughput): ', sum(inf_throughput))\n",
    "  #print('av_throughput: ', av_throughput)\n",
    "  av_fairness_index = sum(inf_fairness_index)/len(inf_fairness_index)\n",
    "  av_task_delay = sum(inf_task_delays)/len(inf_task_delays)\n",
    "  av_local_delay = sum(inf_total_local_delay)/len(inf_total_local_delay)\n",
    "  av_offload_delay = sum(inf_total_offload_delay)/len(inf_total_offload_delay)\n",
    "  av_total_local_queue_length_tasks = sum(inf_total_local_queue_length_tasks)/len(inf_total_local_queue_length_tasks)\n",
    "  av_total_offload_queue_length_tasks = sum(inf_total_offload_queue_length_tasks)/len(inf_total_offload_queue_length_tasks)\n",
    "  av_total_local_queue_length_bits = sum(inf_total_local_queue_length_bits)/len(inf_total_local_queue_length_bits)\n",
    "  av_total_offload_queue_length_bits = sum(inf_total_offload_queue_length_bits)/len(inf_total_offload_queue_length_bits)\n",
    "  #print('len(inf_task_delays): ', len(inf_task_delays))\n",
    "  #print('sum(inf_task_delays): ', sum(inf_task_delays))\n",
    "  av_num_RBs_allocated = sum(inf_num_RBs_allocated)/len(inf_num_RBs_allocated)\n",
    "  inf_outage_probability = [0 if math.isnan(x) else x for x in inf_outage_probability]\n",
    "  av_outage_probability = sum(inf_outage_probability)/len(inf_outage_probability)\n",
    "  av_individual_channel_rates = np.array(inf_individual_channel_rates)\n",
    "  av_individual_num_of_clustered_urllc_users = np.array(inf_individual_num_of_clustered_urllc_users)\n",
    "  #av_individual_channel_rates = np.mean(av_individual_channel_rates, axis=0)\n",
    "  #av_individual_channel_rates = np.mean(av_individual_channel_rates, axis=0)\n",
    "  av_individual_offload_ratios = np.array(inf_individual_offload_ratios)\n",
    "#   print('offloading ratios:')\n",
    "#   print(av_individual_offload_ratios)\n",
    "  av_individual_offload_ratios = np.mean(av_individual_offload_ratios, axis=0)\n",
    "#   print('offloading ratios:')\n",
    "#   print(av_individual_offload_ratios)\n",
    "  av_individual_offload_ratios = np.mean(av_individual_offload_ratios)\n",
    "#   print('offloading ratios:')\n",
    "#   print(av_individual_offload_ratios)\n",
    "\n",
    "  av_individual_local_queue_lengths_bits = np.array(inf_individual_local_queue_lengths_bits)\n",
    "  av_individual_local_queue_lengths_bits = np.mean(av_individual_local_queue_lengths_bits, axis=0)\n",
    "  av_individual_local_queue_lengths_bits = np.mean(av_individual_local_queue_lengths_bits)\n",
    "\n",
    "  av_individual_offload_queue_lengths_bits = np.array(inf_individual_offload_queue_lengths_bits)\n",
    "  av_individual_offload_queue_lengths_bits = np.mean(av_individual_offload_queue_lengths_bits, axis=0)\n",
    "  av_individual_offload_queue_lengths_bits = np.mean(av_individual_offload_queue_lengths_bits)\n",
    "\n",
    "  av_individual_local_queue_lengths_tasks = np.array(inf_individual_local_queue_lengths_tasks)\n",
    "  av_individual_local_queue_lengths_tasks = np.mean(av_individual_local_queue_lengths_tasks, axis=0)\n",
    "  av_individual_local_queue_lengths_tasks = np.mean(av_individual_local_queue_lengths_tasks)\n",
    "\n",
    "  av_individual_offload_queue_lengths_tasks = np.array(inf_individual_offload_queue_lengths_tasks)\n",
    "  av_individual_offload_queue_lengths_tasks = np.mean(av_individual_offload_queue_lengths_tasks, axis=0)\n",
    "  av_individual_offload_queue_lengths_tasks = np.mean(av_individual_offload_queue_lengths_tasks)\n",
    "\n",
    "  av_individual_energy_harvested = np.array(inf_individual_energy_harvested_levels)  \n",
    "  av_individual_energy_harvested = np.mean(av_individual_energy_harvested, axis=0)\n",
    "  av_individual_energy_harvested = np.mean(av_individual_energy_harvested)\n",
    "    \n",
    "  av_individual_battery_energy_levels = np.array(inf_individual_battery_energy_levels)\n",
    "  av_individual_battery_energy_levels = np.mean(av_individual_battery_energy_levels, axis=0)\n",
    "  av_individual_battery_energy_levels = np.mean(av_individual_battery_energy_levels)\n",
    "\n",
    "  av_individual_local_queue_delay_violation_probability = np.array(inf_individual_local_queue_delay_violation_probability)\n",
    "  av_individual_local_queue_delay_violation_probability = np.mean(inf_individual_local_queue_delay_violation_probability, axis=0)\n",
    "  av_individual_local_queue_delay_violation_probability = np.mean(inf_individual_local_queue_delay_violation_probability)\n",
    "\n",
    "  av_individual_offload_queue_delay_violation_probability = np.array(inf_individual_offload_queue_delay_violation_probability)\n",
    "  av_individual_offload_queue_delay_violation_probability = np.mean(inf_individual_offload_queue_delay_violation_probability, axis=0)\n",
    "  av_individual_offload_queue_delay_violation_probability = np.mean(inf_individual_offload_queue_delay_violation_probability)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "  av_individual_number_of_allocated_RB = np.array(inf_individual_number_of_allocated_RB)\n",
    "  #av_individual_number_of_allocated_RB = np.mean(inf_individual_number_of_allocated_RB, axis=0)\n",
    "  #av_individual_number_of_allocated_RB = np.mean(inf_individual_number_of_allocated_RB, axis=0)\n",
    "  #av_individual_number_of_allocated_RB = np.rint(av_individual_number_of_allocated_RB)\n",
    "\n",
    "  av_individual_number_of_puncturing_urllc_users = np.array(inf_individual_number_of_puncturing_urllc_users)\n",
    "  #av_individual_number_of_puncturing_urllc_users = np.mean(inf_individual_number_of_puncturing_urllc_users, axis=0)\n",
    "  #av_individual_number_of_puncturing_urllc_users = np.mean(inf_individual_number_of_puncturing_urllc_users, axis=0)\n",
    "  #av_individual_number_of_puncturing_urllc_users = np.rint(av_individual_number_of_puncturing_urllc_users)\n",
    "\n",
    "  av_inf_failed_urllc_transmissions = sum(inf_failed_urllc_transmissions)/len(inf_failed_urllc_transmissions)\n",
    "\n",
    "  av_battery_energy_constraint_violation_count = np.array(inf_battery_energy_constraint_violation_count)\n",
    "  av_local_queueing_traffic_constraint_violation_count = np.array(inf_local_queueing_traffic_constraint_violation_count)\n",
    "  av_offload_queueing_traffic_constaint_violation_count = np.array(inf_offload_queueing_traffic_constaint_violation_count)\n",
    "  av_local_time_delay_violation_prob_constraint_violation_count = np.array(inf_local_time_delay_violation_prob_constraint_violation_count)\n",
    "  av_offload_time_delay_violation_prob_constraint_violation_count = np.array(inf_offload_time_delay_violation_prob_constraint_violation_count)\n",
    "  av_rmin_constraint_violation_count = np.array(inf_rmin_constraint_violation_count)\n",
    "    \n",
    "  av_battery_energy_constraint_violation_count = sum(inf_battery_energy_constraint_violation_count)/len(inf_battery_energy_constraint_violation_count)\n",
    "  av_local_queueing_traffic_constraint_violation_count = (sum(inf_local_queueing_traffic_constraint_violation_count)/len(inf_local_queueing_traffic_constraint_violation_count))/number_of_users\n",
    "  av_offload_queueing_traffic_constaint_violation_count = (sum(inf_offload_queueing_traffic_constaint_violation_count)/len(inf_offload_queueing_traffic_constaint_violation_count))/number_of_users\n",
    "  av_local_time_delay_violation_prob_constraint_violation_count = (sum(inf_local_time_delay_violation_prob_constraint_violation_count)/len(inf_local_time_delay_violation_prob_constraint_violation_count))/number_of_users\n",
    "  av_offload_time_delay_violation_prob_constraint_violation_count = (sum(inf_offload_time_delay_violation_prob_constraint_violation_count)/len(inf_offload_time_delay_violation_prob_constraint_violation_count))/number_of_users\n",
    "  av_rmin_constraint_violation_count = (sum(inf_rmin_constraint_violation_count)/len(inf_rmin_constraint_violation_count))/number_of_users\n",
    "\n",
    "  np.set_printoptions(threshold=np.inf)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_32), av_reward)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_33), av_energy)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_34), av_throughput)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_35), av_fairness_index)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_36), av_task_delay)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_38), av_individual_channel_rates)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (av_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Energy over the Evaluation Step: %f\" % (av_energy))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Throughput over the Evaluation Step: %f\" % (av_throughput))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Fairness Index over the Evaluation Step: %f\" % (av_fairness_index))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average task delay over the Evaluation Step: %f\" % (av_task_delay))\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average sum local task delay over the Evaluation Step: %f\" % (av_local_delay))\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average sum offload task delay over the Evaluation Step: %f\" % (av_offload_delay))\n",
    "  print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Number of allocated RBs over the Evaluation Step: %f\" % (av_num_RBs_allocated))\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Outage Probability over the Evaluation Step: %f\" % (av_outage_probability))\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Outage Probability over the Evaluation Step: %f\" % (av_inf_failed_urllc_transmissions))\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Individual Channel Rates over the Evaluation Step: \",av_individual_channel_rates[len(av_individual_channel_rates)-1])\n",
    "  # print (\"---------------------------------------\")\n",
    "\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"av_total_local_queue_length_tasks: \",av_total_local_queue_length_tasks)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"av_total_offload_queue_length_tasks: \",av_total_offload_queue_length_tasks)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"av_total_local_queue_length_bits: \",av_total_local_queue_length_bits)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"av_total_offload_queue_length_bits: \",av_total_offload_queue_length_bits)\n",
    "  print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Local Queue Lengths Bits: \",av_individual_local_queue_lengths_bits)\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Offload Queue Lengths Bits: \",av_individual_offload_queue_lengths_bits)\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Local Queue Lengths Tasks: \",av_individual_local_queue_lengths_tasks)\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Offload Queue Lengths Tasks: \",av_individual_offload_queue_lengths_tasks)\n",
    "  # print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Offloading Ratios: \",av_individual_offload_ratios)\n",
    "  print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Individual Number of allocated RBs: \",av_individual_number_of_allocated_RB[len(av_individual_number_of_allocated_RB)-1])\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Individual Number of Punctruring users: \",av_individual_number_of_puncturing_urllc_users[len(av_individual_number_of_puncturing_urllc_users)-1])\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Individual Number of Clustered Urllc users: \",av_individual_num_of_clustered_urllc_users[len(av_individual_num_of_clustered_urllc_users)-1])\n",
    "  # print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average battery_energy_constraint_violation_count: \",av_battery_energy_constraint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average local_queueing_traffic_constraint_violation_count: \",av_local_queueing_traffic_constraint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average offload_queueing_traffic_constaint_violation_count: \",av_offload_queueing_traffic_constaint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average local_time_delay_violation_prob_constraint_violation_count: \",av_local_time_delay_violation_prob_constraint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average offload_time_delay_violation_prob_constraint_violation_count: \",av_offload_time_delay_violation_prob_constraint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average av_rmin_constraint_violation_count: \",av_rmin_constraint_violation_count)\n",
    "  print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Energy Harvested: \",av_individual_energy_harvested)\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"---------------------------------------\")\n",
    "  # print (\"Average Battery Energy Levels: \",av_individual_battery_energy_levels)\n",
    "  # print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Local Queue Delay Violation Probability: \",av_individual_local_queue_delay_violation_probability)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Offload Queue Delay Violation Probability: \",av_individual_offload_queue_delay_violation_probability)\n",
    "  print (\"---------------------------------------\")\n",
    " \n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"NetworkEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 2\n",
    "#env = gym.make(env_name)\n",
    "#env = NetworkEnv_()\n",
    "#max_episode_steps = env._max_episode_steps\n",
    "#if save_env_vid:\n",
    "#  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "#  env.reset()\n",
    "#env.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "#env.SBS1.q_total_local_traffic_reward = 10**9\n",
    "env.change_users_task_arrival_rate(3)\n",
    "print('eMBB user 1 task arrival rate: ', env.eMBB_Users[0].average_task_arrival_rate)\n",
    "number_of_users = len(env.eMBB_Users)\n",
    "state_dim = env.observation_space_dim\n",
    "action_dim = env.action_space_dim\n",
    "max_action = float(env.box_action_space.high[0][1]) # to change this soon\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, 'pytorch_models')\n",
    "_ = evaluate_policy(policy, eval_episodes,number_of_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_DDPG(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor_DDPG, self).__init__()\n",
    "\n",
    "\t\tself.layer_1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.layer_2 = nn.Linear(400, 300)\n",
    "\t\tself.layer_3 = nn.Linear(300, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\n",
    "\t\n",
    "\tdef forward(self, state):\n",
    "\t\tx = F.relu(self.layer_1(state))\n",
    "\t\tx = F.relu(self.layer_2(x))\n",
    "\t\treturn self.max_action * torch.sigmoid(self.layer_3(x))\n",
    "\n",
    "class Critic_DDPG(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic_DDPG, self).__init__()\n",
    "\n",
    "\t\t#self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "\t\t#self.layer_2 = nn.Linear(400, 300)\n",
    "\t\tself.layer_1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.layer_2 = nn.Linear(400 + action_dim, 300)\n",
    "\t\tself.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\t#q = F.relu(self.layer_1(torch.cat([state, action], 1)))\n",
    "\t\t#q = F.relu(self.layer_2(q))\n",
    "\n",
    "\t\tq = F.relu(self.layer_1(state))\n",
    "\t\tq = F.relu(self.layer_2(torch.cat([q, action], 1)))\n",
    "\t\treturn self.layer_3(q)\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  # Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class DDPG(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tself.actor = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.000001)\n",
    "\n",
    "\t\tself.critic = Critic_DDPG(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = Critic_DDPG(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.0001)\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\t#state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\tstate = torch.Tensor(state).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy()\n",
    "\t\t#return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\tdef train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\t\t# Sample replay buffer \n",
    "\t\tbatch_states, batch_next_states, batch_actions,batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "\t\t\n",
    "\t\tstate = torch.Tensor(batch_states).to(device)\n",
    "\t\tnext_state = torch.Tensor(batch_next_states).to(device)\n",
    "\t\taction = torch.Tensor(batch_actions).to(device)\n",
    "\t\treward = torch.Tensor(batch_rewards).to(device)\n",
    "\t\tdone = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "\n",
    "\t\t# Compute the target Q value\n",
    "\t\tnext_action = self.actor_target(next_state)\t\n",
    "\t\ttarget_Q = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "\t\t#target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "\t\ttarget_Q = reward + ((1 - done) *discount * target_Q).detach()\n",
    "\n",
    "\t\t# Get current Q estimate\n",
    "\t\tcurrent_Q = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Compute actor loss\n",
    "\t\tactor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\t\t\n",
    "\t\t# Optimize the actor \n",
    "\t\tself.actor_optimizer.zero_grad()\n",
    "\t\tactor_loss.backward()\n",
    "\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t# Update the frozen target models\n",
    "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename, directory):\n",
    "\t\ttorch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "\t\ttorch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "\tdef load(self, filename, directory):\n",
    "\t\tself.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "\t\tself.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  inf_outage_probability=[]\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(obs)\n",
    "      action = env.reshape_action_space_from_model_to_dict(action)\n",
    "      mode = 'inference'\n",
    "      reformed_action = env.apply_resource_allocation_constraint(action,mode)\n",
    "      obs, reward, done, _ = env.step(reformed_action)\n",
    "      inf_energy.append(env.total_energy)\n",
    "      inf_throughput.append(env.total_rate)\n",
    "      inf_total_reward.append(reward)\n",
    "      inf_fairness_index.append(env.SBS1.fairness_index)\n",
    "      inf_task_delays.append(env.SBS1.total_delay)\n",
    "      inf_num_RBs_allocated.append(env.num_RBs_allocated)\n",
    "      inf_outage_probability.append(env.SBS1.outage_probability)\n",
    "      inf_individual_channel_rates.append(env.SBS1.individual_channel_rates)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "\n",
    "  av_reward = sum(inf_total_reward)/len(inf_total_reward)\n",
    "  av_energy = sum(inf_energy)/len(inf_energy)\n",
    "  av_throughput = sum(inf_throughput)/len(inf_throughput)\n",
    "  av_fairness_index = sum(inf_fairness_index)/len(inf_fairness_index)\n",
    "  av_task_delay = sum(inf_task_delays)/len(inf_task_delays)\n",
    "  av_num_RBs_allocated = sum(inf_num_RBs_allocated)/len(inf_num_RBs_allocated)\n",
    "  inf_outage_probability = [0 if math.isnan(x) else x for x in inf_outage_probability]\n",
    "  av_outage_probability = sum(inf_outage_probability)/len(inf_outage_probability)\n",
    "  av_individual_channel_rates = np.array(inf_individual_channel_rates)\n",
    "  av_individual_channel_rates = np.mean(av_individual_channel_rates, axis=0)\n",
    "\n",
    "  \n",
    "  np.save(\"./inference_results/%s\" % (file_name_32), av_reward)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_33), av_energy)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_34), av_throughput)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_35), av_fairness_index)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_36), av_task_delay)\n",
    "  np.save(\"./inference_results/%s\" % (file_name_38), av_individual_channel_rates)\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Energy over the Evaluation Step: %f\" % (av_energy))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Throughput over the Evaluation Step: %f\" % (av_throughput))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Fairness Index over the Evaluation Step: %f\" % (av_fairness_index))\n",
    "  print (\"---------------------------------------\")\n",
    "  print('')\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average task delay over the Evaluation Step: %f\" % (av_task_delay))\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Number of allocated RBs over the Evaluation Step: %f\" % (av_num_RBs_allocated))\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Outage Probability over the Evaluation Step: %f\" % (av_outage_probability))\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Individual Channel Rates over the Evaluation Step: \",av_individual_channel_rates)\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"NetworkEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "env = gym.make(env_name)\n",
    "#env = NetworkEnv_()\n",
    "#max_episode_steps = env._max_episode_steps\n",
    "#if save_env_vid:\n",
    "#  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "#  env.reset() \n",
    "# #env.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "state_dim = env.observation_space_dim\n",
    "action_dim = env.action_space_dim\n",
    "max_action = float(env.box_action_space.high[0][1]) # to change this soon\n",
    "policy = DDPG(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, 'pytorch_models')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Ant.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
