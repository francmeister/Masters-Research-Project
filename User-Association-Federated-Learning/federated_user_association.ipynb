{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning Github Repository and navigate into project folder\n",
    "!git clone https://github.com/francmeister/Masters-Research-Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Masters-Research-Project/User-Association-Federated-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "from eMBB_UE import eMBB_UE\n",
    "from URLLC_UE import URLLC_UE\n",
    "from SBS import SBS\n",
    "from Communication_Channel import Communication_Channel\n",
    "from numpy import interp\n",
    "import math\n",
    "from NetworkEnv import NetworkEnv\n",
    "from global_entity import GLOBAL_ENTITY\n",
    "from custom_barrier import CustomBarrier\n",
    "from DNN import DNN\n",
    "from DNN_training_memory import DNN_TRAINING_MEMORY\n",
    "from global_entity import GLOBAL_ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a Square grid of overlapping Access Points (3 APs) - 0 to 100 km both x and y\n",
    "\n",
    "x_grid = 5000\n",
    "y_grid = 5000\n",
    "access_point_radius = 4000\n",
    "\n",
    "num_embb_users = 3\n",
    "num_urllc_users = 0\n",
    "all_users = []\n",
    "user_count = 1\n",
    "embb_user_count = 1\n",
    "urllc_user_count = 1\n",
    "access_point_count = 1\n",
    "access_points = []\n",
    "num_access_points = 3\n",
    "num_users = num_embb_users+num_urllc_users\n",
    "num_input_features_per_user = 2\n",
    "num_input_features = num_users*num_input_features_per_user\n",
    "num_output_features = num_users\n",
    "max_samples = 40\n",
    "comm_channel = Communication_Channel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(0,num_embb_users):\n",
    "   embb_user = eMBB_UE(embb_user_count,user_count,100,600)\n",
    "   all_users.append(embb_user)\n",
    "   embb_user_count+=1\n",
    "   user_count+=1\n",
    "\n",
    "for x in range(0,num_urllc_users):\n",
    "   urllc_user = URLLC_UE(urllc_user_count,user_count,100,600)\n",
    "   all_users.append(urllc_user)\n",
    "   urllc_user_count+=1\n",
    "   user_count+=1\n",
    "\n",
    "for x in range(0,num_access_points):\n",
    "   access_point = SBS(access_point_count,num_access_points, num_input_features, num_output_features)\n",
    "   access_points.append(access_point)\n",
    "   access_point_count+=1\n",
    "\n",
    "access_point_coordinates = []\n",
    "for access_point in range(0, num_access_points):\n",
    "   x_coord = random.randint(0, x_grid)\n",
    "   y_coord = random.randint(0, y_grid)\n",
    "   access_point_coordinates.append((x_coord,y_coord))\n",
    "\n",
    "user_coordinates = []\n",
    "for user in range(0, num_users):\n",
    "   x_coord = random.randint(0, x_grid)\n",
    "   y_coord = random.randint(0, y_grid)\n",
    "   user_coordinates.append((x_coord,y_coord))\n",
    "\n",
    "coord_index = 0\n",
    "for access_point in access_points:\n",
    "   access_point.set_coordinates(access_point_coordinates[coord_index])\n",
    "   coord_index+=1\n",
    "\n",
    "coord_index = 0\n",
    "for user in all_users:\n",
    "   user.set_coordinates(user_coordinates[coord_index])\n",
    "   user.calculate_distances_from_access_point(access_point_coordinates,access_point_radius)\n",
    "   coord_index+=1\n",
    "\n",
    "\n",
    "for access_point in access_points:\n",
    "   access_point.find_users_within_distance_radius(access_point_radius, all_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DNN_training_memory.DNN_TRAINING_MEMORY object at 0x0000021B7F234DC0>, <DNN_training_memory.DNN_TRAINING_MEMORY object at 0x0000021B7F234D00>, <DNN_training_memory.DNN_TRAINING_MEMORY object at 0x0000021B7F234910>]\n"
     ]
    }
   ],
   "source": [
    "global_entity = GLOBAL_ENTITY(num_access_points)\n",
    "global_entity.initialize_global_model(num_input_features,num_output_features)\n",
    "global_memory = global_entity.initialize_global_memory(max_samples,num_users,num_input_features_per_user,num_access_points)\n",
    "initial_user_associations = global_entity.perform_random_association(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1.        , 0.21685543, 2.        , 0.36400572, 3.        ,\n",
      "       0.04977668]), array([0.36366338, 0.44389485, 0.06134743]), 3), (array([1.        , 0.3717031 , 2.        , 0.7964079 , 3.        ,\n",
      "       0.03687286]), array([0.43894007, 0.04708377, 0.69257187]), 5), (array([1.        , 0.50753511, 2.        , 0.5904433 , 3.        ,\n",
      "       0.00644526]), array([0.84188319, 0.79149658, 0.28715374]), 3), (array([1.        , 0.34349108, 2.        , 0.56395238, 3.        ,\n",
      "       0.72199894]), array([0.29936349, 0.82733626, 0.84890103]), 1), (array([1.        , 0.24400002, 2.        , 0.37991101, 3.        ,\n",
      "       0.07436551]), array([0.65931749, 0.7475485 , 0.67020306]), 3), (array([1.        , 0.0969542 , 2.        , 0.43222657, 3.        ,\n",
      "       0.9812274 ]), array([0.87981892, 0.97148295, 0.79116369]), 5), (array([1.        , 0.41952692, 2.        , 0.89880624, 3.        ,\n",
      "       0.4435642 ]), array([0.24014126, 0.03415897, 0.99384978]), 2), (array([1.        , 0.93349442, 2.        , 0.63914565, 3.        ,\n",
      "       0.901052  ]), array([0.94078784, 0.23091738, 0.98009251]), 0), (array([1.        , 0.93539761, 2.        , 0.38674245, 3.        ,\n",
      "       0.52704503]), array([0.12302723, 0.40787503, 0.83025201]), 3), (array([1.        , 0.54667775, 2.        , 0.16167901, 3.        ,\n",
      "       0.57331225]), array([0.26968574, 0.49832565, 0.08880793]), 2), (array([1.        , 0.34867348, 2.        , 0.88492916, 3.        ,\n",
      "       0.43839317]), array([0.30903829, 0.21930707, 0.04148109]), 5), (array([1.        , 0.26933079, 2.        , 0.8178582 , 3.        ,\n",
      "       0.84069278]), array([0.65622208, 0.94061693, 0.98218062]), 4), (array([1.        , 0.7913952 , 2.        , 0.28431074, 3.        ,\n",
      "       0.95009883]), array([0.09926778, 0.36358255, 0.91805412]), 1), (array([1.        , 0.2612579 , 2.        , 0.20347831, 3.        ,\n",
      "       0.75276542]), array([0.80144454, 0.77603421, 0.01571605]), 1), (array([1.        , 0.86688659, 2.        , 0.2732859 , 3.        ,\n",
      "       0.63311236]), array([0.44405299, 0.15519871, 0.27398536]), 1), (array([1.        , 0.76188639, 2.        , 0.28513365, 3.        ,\n",
      "       0.64963706]), array([0.64650964, 0.06557832, 0.67071071]), 2), (array([1.        , 0.36946452, 2.        , 0.59757422, 3.        ,\n",
      "       0.77455707]), array([0.13444415, 0.74442997, 0.33459174]), 5), (array([1.        , 0.50214567, 2.        , 0.40372161, 3.        ,\n",
      "       0.00794753]), array([0.81278319, 0.30373184, 0.69889102]), 4), (array([1.        , 0.26000419, 2.        , 0.83829314, 3.        ,\n",
      "       0.62900641]), array([0.9357434 , 0.18345317, 0.59613772]), 1), (array([1.        , 0.10767059, 2.        , 0.63399433, 3.        ,\n",
      "       0.56892222]), array([0.71448911, 0.32604388, 0.44688226]), 4), (array([1.        , 0.48020906, 2.        , 0.51835184, 3.        ,\n",
      "       0.15819749]), array([0.3453377 , 0.90173961, 0.04921059]), 4), (array([1.        , 0.87694907, 2.        , 0.92368403, 3.        ,\n",
      "       0.64027114]), array([0.36624871, 0.80413111, 0.91305998]), 4), (array([1.        , 0.79831486, 2.        , 0.5152032 , 3.        ,\n",
      "       0.89712638]), array([0.0707428 , 0.95105622, 0.06246871]), 1), (array([1.        , 0.66706213, 2.        , 0.09579789, 3.        ,\n",
      "       0.10710472]), array([0.42609544, 0.3323076 , 0.51883974]), 4), (array([1.        , 0.44064764, 2.        , 0.53033078, 3.        ,\n",
      "       0.33385826]), array([0.31450596, 0.63618498, 0.36280025]), 4), (array([1.        , 0.639756  , 2.        , 0.79318405, 3.        ,\n",
      "       0.60130207]), array([0.79212609, 0.43918222, 0.54361822]), 3), (array([1.        , 0.38805104, 2.        , 0.18380287, 3.        ,\n",
      "       0.84578351]), array([0.3338947 , 0.46372119, 0.20814393]), 0), (array([1.        , 0.60678535, 2.        , 0.11740945, 3.        ,\n",
      "       0.82865754]), array([0.5513512 , 0.55897406, 0.56029538]), 2), (array([1.        , 0.11683947, 2.        , 0.34226549, 3.        ,\n",
      "       0.18643861]), array([0.14748776, 0.89766257, 0.66786227]), 0), (array([1.        , 0.4138473 , 2.        , 0.31684381, 3.        ,\n",
      "       0.42820793]), array([0.27028164, 0.76168907, 0.43958399]), 5), (array([1.        , 0.9419745 , 2.        , 0.82661642, 3.        ,\n",
      "       0.05591351]), array([0.63789673, 0.41452046, 0.80423786]), 4), (array([1.        , 0.43570334, 2.        , 0.38254376, 3.        ,\n",
      "       0.81224819]), array([0.96209149, 0.16855536, 0.44128185]), 2), (array([1.        , 0.87130997, 2.        , 0.2994564 , 3.        ,\n",
      "       0.2352594 ]), array([0.95447449, 0.64573184, 0.07059078]), 2), (array([1.        , 0.38867991, 2.        , 0.91258964, 3.        ,\n",
      "       0.78992021]), array([0.16237125, 0.39421623, 0.77973873]), 2), (array([1.        , 0.95196731, 2.        , 0.0378041 , 3.        ,\n",
      "       0.78441141]), array([0.27743341, 0.33162299, 0.00318934]), 0), (array([1.        , 0.02240479, 2.        , 0.40924584, 3.        ,\n",
      "       0.85733577]), array([0.02382682, 0.42887446, 0.73589619]), 4), (array([1.        , 0.24722857, 2.        , 0.02644875, 3.        ,\n",
      "       0.55222392]), array([0.62320499, 0.195257  , 0.59035024]), 1), (array([1.        , 0.44252854, 2.        , 0.26480127, 3.        ,\n",
      "       0.34516113]), array([0.6590382 , 0.65518596, 0.04054684]), 2), (array([1.        , 0.62962729, 2.        , 0.81102074, 3.        ,\n",
      "       0.39096566]), array([0.176646  , 0.1172203 , 0.38381544]), 1), (array([1.        , 0.50098141, 2.        , 0.02894781, 3.        ,\n",
      "       0.22065989]), array([0.54400608, 0.98812108, 0.72857801]), 2)]\n",
      "[(array([1.        , 0.04883877, 2.        , 0.07657408, 3.        ,\n",
      "       0.11653334]), array([0.45131685, 0.87339356, 0.3864521 ]), 4), (array([1.        , 0.52880756, 2.        , 0.318311  , 3.        ,\n",
      "       0.80426664]), array([0.42800508, 0.20478218, 0.31681292]), 4), (array([1.        , 0.73312367, 2.        , 0.20090065, 3.        ,\n",
      "       0.44444551]), array([0.94329839, 0.9570824 , 0.87466196]), 0), (array([1.        , 0.13665214, 2.        , 0.45745314, 3.        ,\n",
      "       0.86124013]), array([0.18094224, 0.18575307, 0.95769487]), 2), (array([1.        , 0.1918245 , 2.        , 0.59713917, 3.        ,\n",
      "       0.47349674]), array([0.80764848, 0.9270611 , 0.50893704]), 1), (array([1.        , 0.85021842, 2.        , 0.16055785, 3.        ,\n",
      "       0.02554086]), array([0.9345533 , 0.94300296, 0.99264151]), 2), (array([1.        , 0.81390757, 2.        , 0.4252167 , 3.        ,\n",
      "       0.44270128]), array([0.81283333, 0.33445908, 0.05834481]), 5), (array([1.        , 0.1614571 , 2.        , 0.93533217, 3.        ,\n",
      "       0.56093438]), array([0.86214936, 0.15308572, 0.40545452]), 5), (array([1.        , 0.40698065, 2.        , 0.31958524, 3.        ,\n",
      "       0.70619732]), array([0.9877216 , 0.21793459, 0.79550212]), 5), (array([1.        , 0.70812579, 2.        , 0.23149336, 3.        ,\n",
      "       0.58947712]), array([0.93532372, 0.84138335, 0.34141802]), 3), (array([1.        , 0.68147641, 2.        , 0.26923406, 3.        ,\n",
      "       0.84210584]), array([0.22508876, 0.03540259, 0.83272029]), 0), (array([1.        , 0.84820444, 2.        , 0.40676895, 3.        ,\n",
      "       0.27802911]), array([0.36564763, 0.48579171, 0.42233712]), 4), (array([1.        , 0.01251381, 2.        , 0.84791554, 3.        ,\n",
      "       0.38371099]), array([0.91808785, 0.30149723, 0.08595304]), 4), (array([1.        , 0.73443394, 2.        , 0.30678613, 3.        ,\n",
      "       0.0932144 ]), array([0.50674416, 0.23551459, 0.31173588]), 0), (array([1.        , 0.62976485, 2.        , 0.22448128, 3.        ,\n",
      "       0.7815415 ]), array([0.87297557, 0.30812452, 0.75548513]), 3), (array([1.        , 0.73905946, 2.        , 0.44023416, 3.        ,\n",
      "       0.59033664]), array([0.75243938, 0.81692659, 0.3113785 ]), 5), (array([1.        , 0.23556655, 2.        , 0.23397125, 3.        ,\n",
      "       0.66716244]), array([0.53678361, 0.61658296, 0.03363937]), 0), (array([1.        , 0.23652363, 2.        , 0.26274786, 3.        ,\n",
      "       0.29869395]), array([0.59611521, 0.72733557, 0.05146888]), 1), (array([1.        , 0.9599382 , 2.        , 0.27913793, 3.        ,\n",
      "       0.11646111]), array([0.38147338, 0.07565174, 0.33397248]), 3), (array([1.        , 0.33452531, 2.        , 0.72222712, 3.        ,\n",
      "       0.9867009 ]), array([0.87615356, 0.57931526, 0.49776259]), 1), (array([1.        , 0.53737976, 2.        , 0.88721072, 3.        ,\n",
      "       0.12954321]), array([0.17260707, 0.74196145, 0.60675067]), 1), (array([1.        , 0.03412024, 2.        , 0.93069432, 3.        ,\n",
      "       0.00720156]), array([0.83286973, 0.96805581, 0.82938456]), 2), (array([1.        , 0.29214435, 2.        , 0.99658971, 3.        ,\n",
      "       0.54277512]), array([0.36201807, 0.75788128, 0.29057339]), 4), (array([1.        , 0.8577757 , 2.        , 0.86690044, 3.        ,\n",
      "       0.00384029]), array([0.57021535, 0.4978358 , 0.34873037]), 3), (array([1.00000000e+00, 9.23266401e-01, 2.00000000e+00, 2.91298040e-01,\n",
      "       3.00000000e+00, 1.25656258e-03]), array([0.71539073, 0.51179341, 0.25502459]), 1), (array([1.        , 0.28642446, 2.        , 0.65502853, 3.        ,\n",
      "       0.32852435]), array([0.24679994, 0.6087237 , 0.1431395 ]), 0), (array([1.        , 0.59156336, 2.        , 0.90397441, 3.        ,\n",
      "       0.62978685]), array([0.53598243, 0.87565846, 0.81704493]), 3), (array([1.        , 0.78450713, 2.        , 0.8890303 , 3.        ,\n",
      "       0.1300093 ]), array([0.06791066, 0.26968728, 0.55397525]), 1), (array([1.        , 0.5984532 , 2.        , 0.36509482, 3.        ,\n",
      "       0.21512102]), array([0.40611435, 0.67083498, 0.18024371]), 2), (array([1.        , 0.44482254, 2.        , 0.50601732, 3.        ,\n",
      "       0.58654135]), array([0.2718675 , 0.89856513, 0.86945314]), 3), (array([1.        , 0.90926933, 2.        , 0.89857073, 3.        ,\n",
      "       0.5397758 ]), array([0.26111594, 0.73624107, 0.09306205]), 1), (array([1.        , 0.65968799, 2.        , 0.80247492, 3.        ,\n",
      "       0.01767375]), array([0.65879894, 0.48602497, 0.22123777]), 5), (array([1.        , 0.43192809, 2.        , 0.25157695, 3.        ,\n",
      "       0.09851502]), array([0.38627518, 0.55315817, 0.55395351]), 5), (array([1.        , 0.107122  , 2.        , 0.15668604, 3.        ,\n",
      "       0.2903219 ]), array([0.19528985, 0.77229553, 0.95246612]), 3), (array([1.        , 0.30491501, 2.        , 0.31296036, 3.        ,\n",
      "       0.50609926]), array([0.89796554, 0.9925617 , 0.85036516]), 5), (array([1.        , 0.36365978, 2.        , 0.63206154, 3.        ,\n",
      "       0.74688268]), array([0.37082198, 0.06610829, 0.1949532 ]), 1), (array([1.        , 0.70899472, 2.        , 0.44716509, 3.        ,\n",
      "       0.00410779]), array([0.54553998, 0.92697707, 0.23114461]), 2), (array([1.        , 0.9477845 , 2.        , 0.09413893, 3.        ,\n",
      "       0.13097321]), array([0.41829897, 0.33906711, 0.3653809 ]), 5), (array([1.        , 0.39823345, 2.        , 0.11352873, 3.        ,\n",
      "       0.24469684]), array([0.56458467, 0.93764648, 0.43337813]), 1), (array([1.        , 0.66076994, 2.        , 0.28723088, 3.        ,\n",
      "       0.81364542]), array([0.90543499, 0.80787584, 0.48607006]), 5)]\n",
      "[(array([1.        , 0.25820804, 2.        , 0.1642345 , 3.        ,\n",
      "       0.6054007 ]), array([0.72563055, 0.85689611, 0.97552202]), 4), (array([1.        , 0.87055151, 2.        , 0.19769498, 3.        ,\n",
      "       0.14853577]), array([0.49327534, 0.06707187, 0.49146433]), 4), (array([1.        , 0.72131558, 2.        , 0.15993543, 3.        ,\n",
      "       0.79322973]), array([0.69268865, 0.41277578, 0.4580551 ]), 5), (array([1.        , 0.43111166, 2.        , 0.26127862, 3.        ,\n",
      "       0.96404278]), array([0.9876126 , 0.04015975, 0.0106485 ]), 0), (array([1.        , 0.41861561, 2.        , 0.23210707, 3.        ,\n",
      "       0.74780775]), array([0.51764004, 0.18130825, 0.36595692]), 2), (array([1.        , 0.01842928, 2.        , 0.61650491, 3.        ,\n",
      "       0.57825956]), array([0.35561671, 0.26651622, 0.09411245]), 5), (array([1.        , 0.06218759, 2.        , 0.29465943, 3.        ,\n",
      "       0.07169264]), array([0.63770918, 0.50330239, 0.50606937]), 3), (array([1.        , 0.5241834 , 2.        , 0.45297002, 3.        ,\n",
      "       0.11819495]), array([0.12686342, 0.23979839, 0.09587781]), 5), (array([1.        , 0.48974077, 2.        , 0.66355633, 3.        ,\n",
      "       0.8408573 ]), array([0.14899564, 0.74076232, 0.91651839]), 2), (array([1.        , 0.36915535, 2.        , 0.76915629, 3.        ,\n",
      "       0.01457344]), array([0.47484156, 0.31450243, 0.85105693]), 1), (array([1.        , 0.77517767, 2.        , 0.21268592, 3.        ,\n",
      "       0.65537068]), array([0.82714565, 0.69278871, 0.82927863]), 0), (array([1.        , 0.36603892, 2.        , 0.4286366 , 3.        ,\n",
      "       0.57904921]), array([0.54322339, 0.98238362, 0.88152654]), 3), (array([1.        , 0.73293259, 2.        , 0.02084949, 3.        ,\n",
      "       0.68224772]), array([0.18169751, 0.19460234, 0.1683231 ]), 0), (array([1.        , 0.02599819, 2.        , 0.51118602, 3.        ,\n",
      "       0.47281167]), array([0.46972571, 0.14860443, 0.83196455]), 5), (array([1.        , 0.48726737, 2.        , 0.18364372, 3.        ,\n",
      "       0.60279943]), array([0.40345859, 0.45203257, 0.01041503]), 1), (array([1.        , 0.46215659, 2.        , 0.55325375, 3.        ,\n",
      "       0.1157967 ]), array([0.31541117, 0.46958598, 0.93910187]), 1), (array([1.        , 0.26225079, 2.        , 0.3620964 , 3.        ,\n",
      "       0.15836225]), array([0.58086498, 0.50362781, 0.98157719]), 4), (array([1.        , 0.70708786, 2.        , 0.71377966, 3.        ,\n",
      "       0.92222767]), array([0.36391705, 0.58833865, 0.59573102]), 3), (array([1.        , 0.30608612, 2.        , 0.43457892, 3.        ,\n",
      "       0.92967351]), array([0.5562207 , 0.43386484, 0.34764283]), 1), (array([1.        , 0.44590548, 2.        , 0.97092485, 3.        ,\n",
      "       0.17391434]), array([0.8409129 , 0.58985364, 0.26244619]), 1), (array([1.        , 0.87019955, 2.        , 0.31262369, 3.        ,\n",
      "       0.47530698]), array([0.36382995, 0.53253857, 0.17855271]), 4), (array([1.        , 0.51560592, 2.        , 0.75389088, 3.        ,\n",
      "       0.55672685]), array([0.52464833, 0.98050281, 0.38281863]), 0), (array([1.        , 0.58988493, 2.        , 0.06371498, 3.        ,\n",
      "       0.02627133]), array([0.19845486, 0.07562583, 0.89815635]), 3), (array([1.        , 0.09749239, 2.        , 0.23882678, 3.        ,\n",
      "       0.04841173]), array([0.0670382 , 0.89050414, 0.50171856]), 3), (array([1.        , 0.94285942, 2.        , 0.2138616 , 3.        ,\n",
      "       0.12418501]), array([0.50199449, 0.67101821, 0.99515619]), 2), (array([1.        , 0.58839467, 2.        , 0.48955633, 3.        ,\n",
      "       0.91103084]), array([0.91407929, 0.662484  , 0.86599631]), 0), (array([1.        , 0.49984189, 2.        , 0.77853746, 3.        ,\n",
      "       0.61737095]), array([0.49358559, 0.49346684, 0.42805937]), 2), (array([1.        , 0.76274897, 2.        , 0.55347173, 3.        ,\n",
      "       0.34583082]), array([0.75724963, 0.43520901, 0.708695  ]), 2), (array([1.        , 0.34443925, 2.        , 0.04523564, 3.        ,\n",
      "       0.29993682]), array([0.60307977, 0.03115061, 0.68255248]), 5), (array([1.        , 0.09928697, 2.        , 0.87742061, 3.        ,\n",
      "       0.99464062]), array([0.92800129, 0.58517295, 0.7178981 ]), 3), (array([1.        , 0.75037047, 2.        , 0.84559235, 3.        ,\n",
      "       0.43006579]), array([0.05261525, 0.2344089 , 0.9190052 ]), 0), (array([1.        , 0.95214554, 2.        , 0.79157249, 3.        ,\n",
      "       0.98578182]), array([0.61281898, 0.1607429 , 0.66804559]), 0), (array([1.        , 0.61191563, 2.        , 0.01994048, 3.        ,\n",
      "       0.24220774]), array([0.07066608, 0.1489527 , 0.34074004]), 1), (array([1.        , 0.891758  , 2.        , 0.83141677, 3.        ,\n",
      "       0.64667284]), array([0.24111681, 0.92281849, 0.2843985 ]), 1), (array([1.        , 0.20264421, 2.        , 0.838878  , 3.        ,\n",
      "       0.65334049]), array([0.45554595, 0.35576999, 0.56449746]), 2), (array([1.        , 0.94736749, 2.        , 0.27214988, 3.        ,\n",
      "       0.40002703]), array([0.84300726, 0.92216995, 0.75994971]), 2), (array([1.        , 0.43687189, 2.        , 0.6118894 , 3.        ,\n",
      "       0.18643502]), array([0.05251903, 0.92741382, 0.20655993]), 0), (array([1.        , 0.09954889, 2.        , 0.14120868, 3.        ,\n",
      "       0.12121358]), array([0.04976306, 0.50675997, 0.11858616]), 2), (array([1.        , 0.45274662, 2.        , 0.93144591, 3.        ,\n",
      "       0.45000973]), array([0.63897179, 0.23043804, 0.62483602]), 2), (array([1.        , 0.78636109, 2.        , 0.19894867, 3.        ,\n",
      "       0.59739344]), array([0.38290081, 0.52875024, 0.37996855]), 5)]\n"
     ]
    }
   ],
   "source": [
    "for access_point in access_points:\n",
    "   access_point.get_all_users(all_users)\n",
    "   access_point.initialize_DNN_model(global_entity.global_model)\n",
    "   access_point.acquire_global_memory(global_entity.global_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n",
      "(1, (1, 2.674246435914237e-07))\n",
      "(2, (2, 0.025472243717427012))\n",
      "(3, (2, 0.01822491975290975))\n"
     ]
    }
   ],
   "source": [
    "for access_point in access_points:\n",
    "   access_point_users = []\n",
    "   for user in all_users:\n",
    "      for association in initial_user_associations:\n",
    "         #print(association)\n",
    "         if association[0] == user.user_label and association[1][0] == access_point.SBS_label:\n",
    "            user.distance_from_associated_access_point = association[1][1]\n",
    "            access_point_users.append(user)\n",
    "\n",
    "   access_point.associate_users(access_point_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind:\n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.sigmoid(self.layer_3(x))\n",
    "    #x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.0000001)\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.0001)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    #return self.actor(state).cpu().data.numpy().flatten()\n",
    "    return self.actor(state).cpu().data.numpy()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "    for it in range(iterations):\n",
    "\n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "\n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
    "\n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, access_point_id,eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(obs)\n",
    "      action = env.reshape_action_space_from_model_to_dict(action)\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward#interp(sum(reward),[720000000,863000000],[0,1000])\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Access Point: %i. Average Reward over the Evaluation Step: %f\" % (access_point_id,avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(policy,replay_buffer,env, access_point_id,access_point_radius,barrier):\n",
    "    access_point_id = access_point_id\n",
    "    #print('env')\n",
    "    #print(env)\n",
    "    env_name = \"Access Point \" + str(access_point_id) # Name of a environment (set it to any Continous environment you want)\n",
    "    #print('env name')\n",
    "    #print(env_name)\n",
    "    seed = 0 # Random seed number\n",
    "    start_timesteps = 50000 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "    start_timesteps_global = start_timesteps/2\n",
    "    eval_freq = 5000 # How often the evaluation step is performed (after how many timesteps)\n",
    "    max_timesteps = 500000 # Total number of iterations/timesteps\n",
    "    save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "    expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "    batch_size = 100 # Size of the batch\n",
    "    discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "    tau = 0.005 # Target network update rate\n",
    "    policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "    noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "    policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
    "\n",
    "\n",
    "    results_folder_name = \"./results/access_point_%d\" % (access_point_id)\n",
    "    model_save_folder_name = \"./pytorch_models/access_point_%d\" % (access_point_id)\n",
    "\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name)\n",
    "    if save_models and not os.path.exists(model_save_folder_name):\n",
    "        os.makedirs(model_save_folder_name)\n",
    "\n",
    "    file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "    file_name_0 = \"Evaluations\"\n",
    "    file_name_1 = \"timestep_rewards_energy_throughput\"\n",
    "    file_name_2 = \"offloading_actions\"\n",
    "    file_name_3 = \"power_actions\"\n",
    "    file_name_4 = \"subcarrier_actions\"\n",
    "    file_name_5 = \"allocated_RBs\"\n",
    "    file_name_6 = \"fairnes_index\"\n",
    "\n",
    "    file_name_7 = \"energy_efficiency_rewards\"\n",
    "    file_name_8 = \"battery_energy_rewards\"\n",
    "    file_name_9 = \"throughput_rewards\"\n",
    "    file_name_10 = \"delay_rewards\"\n",
    "    file_name_11 = \"sum_allocations_per_RB_matrix\"\n",
    "    file_name_12 = \"RB_allocation_matrix\"\n",
    "    file_name_13 = \"energy_rewards\"\n",
    "    file_name_14 = \"delays\"\n",
    "    file_name_15 = \"tasks_dropped\"\n",
    "    file_name_16 = \"resource_allocation_matrix\"\n",
    "    file_name_17 = \"resource_allocation_constraint_violation_count\"\n",
    "    file_name_18 = \"global_reward\"\n",
    "    file_name_19 = \"average_reward_in_memory\"\n",
    "    file_name_20 = \"DNN_training_loss\"\n",
    "\n",
    "    evaluations = []#[evaluate_policy(policy, env, access_point_id, user_association_number)]\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    done = True\n",
    "    t0 = time.time()\n",
    "    timestep_rewards = []\n",
    "    timestep_rewards_energy_throughput = []\n",
    "    offload_actions = []\n",
    "    power_actions = []\n",
    "    subcarrier_actions = []\n",
    "    allocated_RBs = []\n",
    "    fairness_index = []\n",
    "    energy_efficiency_rewards = []\n",
    "    battery_energy_rewards = []\n",
    "    energy_rewards = []\n",
    "    throughput_rewards = []\n",
    "    delay_rewards = []\n",
    "    sum_allocations_per_RB_matrix = []\n",
    "    change_action = 0\n",
    "    RB_allocation_matrix = []\n",
    "    delays = []\n",
    "    tasks_dropped = []\n",
    "    resource_allocation_matrix = []\n",
    "    resource_allocation_constraint_violation_count = []\n",
    "    global_reward = []\n",
    "    average_reward_in_memory = []\n",
    "    # We start the main loop over 500,000 timesteps\n",
    "    while total_timesteps < max_timesteps:\n",
    "\n",
    "    # If the episode is done\n",
    "        if done:\n",
    "            # If we are not at the very beginning, we start the training process of the model\n",
    "            if total_timesteps != 0:\n",
    "                print(\"Access Point: {} Total Timesteps: {} Episode Num: {} Reward: {} User_Association_Reward: {}\".format(access_point_id,total_timesteps, episode_num, env.episode_reward, env.user_association_channel_rate_reward))\n",
    "                timestep_rewards.append([total_timesteps, episode_reward])\n",
    "                timestep_rewards_energy_throughput.append([total_timesteps,episode_reward,env.total_energy,env.total_rate])\n",
    "                offload_actions.append(env.offload_decisions)\n",
    "                power_actions.append(env.powers)\n",
    "                subcarrier_actions.append(env.subcarriers)\n",
    "                allocated_RBs.append(env.Communication_Channel_1.allocated_RBs)\n",
    "                fairness_index.append(env.SBS.fairness_index)\n",
    "\n",
    "                energy_efficiency_rewards.append(env.SBS.energy_efficiency_rewards)\n",
    "                battery_energy_rewards.append(env.SBS.battery_energy_rewards)\n",
    "                throughput_rewards.append(env.SBS.throughput_rewards)\n",
    "                delay_rewards.append(env.SBS.delay_rewards)\n",
    "                sum_allocations_per_RB_matrix.append(env.sum_allocations_per_RB_matrix)\n",
    "                RB_allocation_matrix.append(env.RB_allocation_matrix)\n",
    "                energy_rewards.append(env.SBS.energy_rewards)\n",
    "                delays.append(env.SBS.delays)\n",
    "                tasks_dropped.append(env.SBS.tasks_dropped)\n",
    "                resource_allocation_matrix.append(env.resource_block_allocation_matrix)\n",
    "                resource_allocation_constraint_violation_count.append(env.resource_allocation_constraint_violation)\n",
    "                average_reward_in_memory.append(env.SBS.average_reward_in_memory)\n",
    "\n",
    "                policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "            # We evaluate the episode and we save the policy\n",
    "            if timesteps_since_eval >= eval_freq:\n",
    "                timesteps_since_eval %= eval_freq\n",
    "                evaluations.append(evaluate_policy(policy,env, access_point_id))\n",
    "                policy.save(file_name, directory=\"./pytorch_models/access_point_%d\" % (access_point_id))\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_0), evaluations)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_1), timestep_rewards_energy_throughput)\n",
    "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_2), offload_actions)\n",
    "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_3), power_actions)\n",
    "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_4), subcarrier_actions)\n",
    "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_5), allocated_RBs)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_6), fairness_index)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_7), energy_efficiency_rewards)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_8), battery_energy_rewards)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_9), throughput_rewards)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_10), delay_rewards)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_11), sum_allocations_per_RB_matrix)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_12), RB_allocation_matrix)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_13), energy_rewards)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_14), delays)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_15), tasks_dropped)\n",
    "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_16), resource_allocation_matrix)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_17), resource_allocation_constraint_violation_count)\n",
    "\n",
    "            # When the training step is done, we reset the state of the environment\n",
    "            if total_timesteps != 0:\n",
    "                if total_timesteps >= 50000:#start_timesteps_global:\n",
    "                    local_model = env.SBS.train_local_dnn()\n",
    "                    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_20), env.SBS.training_loss)\n",
    "                    barrier.wait_for_aggregation(global_entity,local_model, access_point_id)\n",
    "                else:\n",
    "                    barrier.wait_for_aggregation(global_entity,env.SBS.access_point_model, access_point_id)\n",
    "\n",
    "                barrier.wait_for_reassociations(env, global_entity, total_timesteps, user_association_episode_reward, access_point_radius)\n",
    "                global_reward.append(global_entity.global_reward)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_18), global_reward)\n",
    "                user_association = global_entity.aggregate_user_associations()\n",
    "                #print('aggregated association: ', user_association)\n",
    "                #print('----------user_association-------------', user_association)\n",
    "                #print('SBS label: ', env.SBS.SBS_label)\n",
    "                env.SBS.reassociate_users(user_association)\n",
    "                env.SBS.populate_buffer_memory_sample_with_reward(global_entity.global_reward)\n",
    "                np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_19), average_reward_in_memory)\n",
    "                global_entity.clear_local_models_memory()\n",
    "                global_entity.clear_local_user_associations()\n",
    "                global_entity.reset_global_reward()\n",
    "            obs = env.reset()\n",
    "\n",
    "            # Set the Done to False\n",
    "            done = False\n",
    "\n",
    "            # Set rewards and episode timesteps to zero\n",
    "            episode_reward = 0\n",
    "            user_association_episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        # Before 10000 timesteps, we play random actions\n",
    "        if total_timesteps < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "            action = env.enforce_constraint(action)\n",
    "            action2, action = env.reshape_action_space_dict(action)\n",
    "\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            user_association_reward = env.user_association_channel_rate_reward\n",
    "\n",
    "        else: # After 10000 timesteps, we switch to the model\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "                action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape)).clip(env.action_space_low, env.action_space_high)\n",
    "\n",
    "            #print(action)\n",
    "            action = env.reshape_action_space_from_model_to_dict(action)\n",
    "            reformed_action = env.apply_resource_allocation_constraint(action)\n",
    "\n",
    "            new_obs, reward, done, _ = env.step(reformed_action)\n",
    "            user_association_reward = env.user_association_channel_rate_reward\n",
    "\n",
    "        #print(\"Action in training\")\n",
    "        #print(action)\n",
    "        #print(' ')\n",
    "        # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "        # We check if the episode is done\n",
    "        done_bool = 0 if episode_timesteps + 1 == env.STEP_LIMIT else float(done)\n",
    "\n",
    "        # We increase the total reward\n",
    "        episode_reward += reward\n",
    "        user_association_episode_reward+=user_association_reward\n",
    "        #print('episode reward')\n",
    "        #print(episode_reward)\n",
    "        #episode_reward = interp(episode_reward,[720000000,863000000],[0,1000])\n",
    "        # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "        action = env.reshape_action_space_for_model(action)\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "        # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "        obs = new_obs\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1\n",
    "\n",
    "        # We add the last policy evaluation to our list of evaluations and we save our model\n",
    "    evaluations.append(evaluate_policy(policy,env, access_point_id))\n",
    "    if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models/access_point_%d\" % (access_point_id))\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_0), evaluations)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_1), timestep_rewards_energy_throughput)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_2), offload_actions)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_3), power_actions)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_4), subcarrier_actions)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_5), allocated_RBs)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_6), fairness_index)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_7), energy_efficiency_rewards)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_8), battery_energy_rewards)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_9), throughput_rewards)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_10), delay_rewards)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_11), sum_allocations_per_RB_matrix)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_12), RB_allocation_matrix)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_13), energy_rewards)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_14), delays)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_15), tasks_dropped)\n",
    "    #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_16), resource_allocation_matrix)\n",
    "    np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_17), resource_allocation_constraint_violation_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrier = CustomBarrier(num_access_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_points_envs = []\n",
    "access_point_users = []\n",
    "#user_association_epoch_number = i\n",
    "policies = []\n",
    "replay_buffers = []\n",
    "threadlist = []\n",
    "\n",
    "state_dim = 0\n",
    "action_dim = 0\n",
    "max_action = 0\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,num_access_points+1):\n",
    "    env_name = \"AccessPoint\" + str(x) + \"-v0\"\n",
    "    env = NetworkEnv(all_users,access_points[x-1],access_point_coordinates,access_point_radius)        \n",
    "    access_points_envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,num_access_points):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    state_dim = access_points_envs[x].observation_space_dim\n",
    "    action_dim = access_points_envs[x].action_space_dim\n",
    "    max_action = float(access_points_envs[x].box_action_space.high[0][1]) # to change this soon\n",
    "\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    policies.append(policy)\n",
    "    replay_buffers.append(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,num_access_points):\n",
    "    t = threading.Thread(target=training,args=(policies[x],replay_buffers[x],access_points_envs[x],x+1,access_point_radius,barrier))\n",
    "    print('inner thread id: ', t.name)\n",
    "    threadlist.append(t)\n",
    "    #t.start()\n",
    "\n",
    "for t in threadlist:\n",
    "    t.start()\n",
    "for tr in threadlist:\n",
    "    tr.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
