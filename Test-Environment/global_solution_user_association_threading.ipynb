{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cloning Github Repository and navigate into project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloning Github Repository and navigate into project folder\n",
        "!git clone https://github.com/francmeister/Masters-Research-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cd Masters-Research-Project/Test-Environment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONSzlckBt4DJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import gym\n",
        "import time\n",
        "import threading\n",
        "from eMBB_UE import eMBB_UE\n",
        "from URLLC_UE import URLLC_UE\n",
        "from numpy import interp\n",
        "import math\n",
        "from NetworkEnv import NetworkEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPsHjdfyuC9z"
      },
      "source": [
        "# Initializing User Association DNN\n",
        "## Defining the deep neural network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoqB0OAQuUIL"
      },
      "outputs": [],
      "source": [
        "class UserAssociationDNN(nn.Module):\n",
        "     def __init__(self, input_dim, output_dim):\n",
        "         super(UserAssociationDNN, self).__init__()\n",
        "         self.fc1 = nn.Linear(input_dim, 64)\n",
        "         self.relu = nn.ReLU()\n",
        "         self.fc2 = nn.Linear(64, 32)\n",
        "         self.fc3 = nn.Linear(32, output_dim)\n",
        "         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "     def forward(self, x):\n",
        "         x = self.relu(self.fc1(x))\n",
        "         x = self.relu(self.fc2(x))\n",
        "         x = self.sigmoid(self.fc3(x))\n",
        "         return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqFVS78Puo9z"
      },
      "source": [
        "# Define some environment varibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Is1JaQqurVD"
      },
      "outputs": [],
      "source": [
        "num_embb_users = 4\n",
        "num_urllc_users = 6\n",
        "user_count = 1\n",
        "embb_user_count = 1\n",
        "urllc_user_count = 1\n",
        "all_users = []\n",
        "num_access_points = 3\n",
        "num_users = num_embb_users+num_urllc_users\n",
        "num_batches = 2#num_users*5\n",
        "num_task_arrival_rate = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Kxi0fVuuBw"
      },
      "source": [
        "# Create user objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weBrFCzouv-M"
      },
      "outputs": [],
      "source": [
        "for x in range(0,num_embb_users):\n",
        "   embb_user = eMBB_UE(embb_user_count,user_count,100,600)\n",
        "   all_users.append(embb_user)\n",
        "   embb_user_count+=1\n",
        "   user_count+=1\n",
        "\n",
        "for x in range(0,num_urllc_users):\n",
        "   urllc_user = URLLC_UE(urllc_user_count,user_count,100,600)\n",
        "   all_users.append(urllc_user)\n",
        "   urllc_user_count+=1\n",
        "   user_count+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl9xSlswuy1v"
      },
      "source": [
        "# Functions for producing a set of user association schemes based on random exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIKv6a-Ou1L0"
      },
      "outputs": [],
      "source": [
        "def random_exploration(num_users,num_access_points,num_batches):\n",
        "   user_association_labels = []\n",
        "   user_association_labels_ = []\n",
        "   count = 0\n",
        "   print('num_batches: ', num_batches)\n",
        "   while count < num_batches:\n",
        "      user_association_labels = np.random.randint(2, size=(num_users, num_access_points))\n",
        "\n",
        "      if np.all(np.sum(user_association_labels,axis=1) == 1):\n",
        "         user_association_labels_.append(user_association_labels)\n",
        "         count+=1\n",
        "\n",
        "   user_association_labels_ = np.array(user_association_labels_)\n",
        "   user_association_labels_for_model_training = user_association_labels_.reshape(num_batches,num_access_points*num_users)\n",
        "   print('len(user_association_labels_for_model_training): ', len(user_association_labels_for_model_training))\n",
        "   return user_association_labels_,user_association_labels_for_model_training\n",
        "\n",
        "\n",
        "def generate_user_association_schemes(users,num_users,num_access_points,num_batches):\n",
        "   user_association_labels, user_association_labels_for_model_training = random_exploration(num_users,num_access_points,num_batches)\n",
        "\n",
        "   for user_association_scheme in user_association_labels:\n",
        "      count = 0\n",
        "      for user in all_users:\n",
        "         user.assigned_access_point_label_matrix.append(user_association_scheme[count])\n",
        "         count+=1\n",
        "\n",
        "   for user in all_users:\n",
        "      user.assigned_access_point_label_matrix_to_numpy_array()\n",
        "\n",
        "   return users, user_association_labels, user_association_labels_for_model_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IW7iilJu9eP"
      },
      "source": [
        "# Initializing DRL model classes\n",
        "## Experience Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTqmMBBHu_pj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhcdX_XIvJEW"
      },
      "source": [
        "# Actor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z80AHlF3vKLD"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.sigmoid(self.layer_3(x))\n",
        "    #x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqmFqfJOuJSL"
      },
      "source": [
        "# Critic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVqkSqHdvUct"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNqYnQCWvaB9"
      },
      "source": [
        "# TD3 Training class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmUXfV8eyHQS"
      },
      "source": [
        "# Selecting the device (CPU or GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCBESVPByJ7Z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQLaqcAOyNkg"
      },
      "source": [
        "# Building the whole Training Process into a class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg9wvQT7yOth"
      },
      "outputs": [],
      "source": [
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.0000001)\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.0001)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state).to(device)\n",
        "    #return self.actor(state).cpu().data.numpy().flatten()\n",
        "    return self.actor(state).cpu().data.numpy()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      batch_states = np.reshape(batch_states,(batch_states.shape[0]*batch_states.shape[1],batch_states.shape[2]))\n",
        "      batch_next_states = np.reshape(batch_next_states,(batch_next_states.shape[0]*batch_next_states.shape[1],batch_next_states.shape[2]))\n",
        "      batch_actions = np.reshape(batch_actions,(batch_actions.shape[0]*batch_actions.shape[1],batch_actions.shape[2]))\n",
        "\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, torch.Tensor(next_action).to(device))\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgLwpMDbyViA"
      },
      "source": [
        "# Function to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT0QWhSTyXbD"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, env, access_point_id,user_association_number,eval_episodes=1):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      action = env.reshape_action_space_from_model_to_dict(action)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      done = done[len(done)-1]\n",
        "      avg_reward += sum(reward)#interp(sum(reward),[720000000,863000000],[0,1000])\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Access Point: %i. User association number: %i. Average Reward over the Evaluation Step: %f\" % (access_point_id,user_association_number,avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdYLcDZGyaYr"
      },
      "source": [
        "# Function to assign users to their allocated access points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXqIwpXSycmQ"
      },
      "outputs": [],
      "source": [
        "def find_access_point_users(users,access_point_id,user_association_epoch_number):\n",
        "   access_point_users = []\n",
        "   for user in users:\n",
        "      if user.assigned_access_point_label_matrix_integers[user_association_epoch_number] == access_point_id:\n",
        "         access_point_users.append(user)\n",
        "   return access_point_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orIeoKsygCf"
      },
      "source": [
        "# Function to perform training on the DRL model(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbdlhFT-yhGw"
      },
      "outputs": [],
      "source": [
        "def training(policy,replay_buffer,env, access_point_id, user_association_number):\n",
        "    access_point_id = access_point_id+1\n",
        "    #print('env')\n",
        "    #print(env)\n",
        "    env_name = \"Access Point \" + str(access_point_id) # Name of a environment (set it to any Continous environment you want)\n",
        "    #print('env name')\n",
        "    #print(env_name)\n",
        "    seed = 0 # Random seed number\n",
        "    start_timesteps = 10000 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "    eval_freq = 5000 # How often the evaluation step is performed (after how many timesteps)\n",
        "    max_timesteps = 500000 # Total number of iterations/timesteps\n",
        "    save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "    expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "    batch_size = 100 # Size of the batch\n",
        "    discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "    tau = 0.005 # Target network update rate\n",
        "    policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "    noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "    policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "\n",
        "\n",
        "    results_folder_name = \"./results/access_point_%d_user_association_%i\" % (access_point_id,user_association_number)\n",
        "    model_save_folder_name = \"./pytorch_models/access_point_%d_user_association_%i\" % (access_point_id,user_association_number)\n",
        "\n",
        "    if not os.path.exists(results_folder_name):\n",
        "        os.makedirs(results_folder_name)\n",
        "    if save_models and not os.path.exists(model_save_folder_name):\n",
        "        os.makedirs(model_save_folder_name)\n",
        "\n",
        "    file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "    file_name_0 = \"Evaluations\"\n",
        "    file_name_1 = \"timestep_rewards_energy_throughput\"\n",
        "    file_name_2 = \"offloading_actions\"\n",
        "    file_name_3 = \"power_actions\"\n",
        "    file_name_4 = \"subcarrier_actions\"\n",
        "    file_name_5 = \"allocated_RBs\"\n",
        "    file_name_6 = \"fairnes_index\"\n",
        "\n",
        "    file_name_7 = \"energy_efficiency_rewards\"\n",
        "    file_name_8 = \"battery_energy_rewards\"\n",
        "    file_name_9 = \"throughput_rewards\"\n",
        "    file_name_10 = \"delay_rewards\"\n",
        "    file_name_11 = \"sum_allocations_per_RB_matrix\"\n",
        "    file_name_12 = \"RB_allocation_matrix\"\n",
        "    file_name_13 = \"energy_rewards\"\n",
        "    file_name_14 = \"delays\"\n",
        "    file_name_15 = \"tasks_dropped\"\n",
        "    file_name_16 = \"resource_allocation_matrix\"\n",
        "    file_name_17 = \"resource_allocation_constraint_violation_count\"\n",
        "\n",
        "    evaluations = []#[evaluate_policy(policy, env, access_point_id, user_association_number)]\n",
        "    total_timesteps = 0\n",
        "    timesteps_since_eval = 0\n",
        "    episode_num = 0\n",
        "    done = True\n",
        "    t0 = time.time()\n",
        "    timestep_rewards = []\n",
        "    timestep_rewards_energy_throughput = []\n",
        "    offload_actions = []\n",
        "    power_actions = []\n",
        "    subcarrier_actions = []\n",
        "    allocated_RBs = []\n",
        "    fairness_index = []\n",
        "    energy_efficiency_rewards = []\n",
        "    battery_energy_rewards = []\n",
        "    energy_rewards = []\n",
        "    throughput_rewards = []\n",
        "    delay_rewards = []\n",
        "    sum_allocations_per_RB_matrix = []\n",
        "    change_action = 0\n",
        "    RB_allocation_matrix = []\n",
        "    delays = []\n",
        "    tasks_dropped = []\n",
        "    resource_allocation_matrix = []\n",
        "    resource_allocation_constraint_violation_count = []\n",
        "    # We start the main loop over 500,000 timesteps\n",
        "    while total_timesteps < max_timesteps:\n",
        "\n",
        "    # If the episode is done\n",
        "        if done:\n",
        "            # If we are not at the very beginning, we start the training process of the model\n",
        "            if total_timesteps != 0:\n",
        "                print(\"User Association Number: {} Access Point: {} Total Timesteps: {} Episode Num: {} Reward: {}\".format(user_association_number,access_point_id,total_timesteps, episode_num, episode_reward))\n",
        "                timestep_rewards.append([total_timesteps, episode_reward])\n",
        "                timestep_rewards_energy_throughput.append([total_timesteps,episode_reward,env.total_energy,env.total_rate])\n",
        "                offload_actions.append(env.offload_decisions)\n",
        "                power_actions.append(env.powers)\n",
        "                subcarrier_actions.append(env.subcarriers)\n",
        "                allocated_RBs.append(env.Communication_Channel_1.allocated_RBs)\n",
        "                fairness_index.append(env.SBS1.fairness_index)\n",
        "\n",
        "                energy_efficiency_rewards.append(env.SBS1.energy_efficiency_rewards)\n",
        "                battery_energy_rewards.append(env.SBS1.battery_energy_rewards)\n",
        "                throughput_rewards.append(env.SBS1.throughput_rewards)\n",
        "                delay_rewards.append(env.SBS1.delay_rewards)\n",
        "                sum_allocations_per_RB_matrix.append(env.sum_allocations_per_RB_matrix)\n",
        "                RB_allocation_matrix.append(env.RB_allocation_matrix)\n",
        "                energy_rewards.append(env.SBS1.energy_rewards)\n",
        "                delays.append(env.SBS1.delays)\n",
        "                tasks_dropped.append(env.SBS1.tasks_dropped)\n",
        "                resource_allocation_matrix.append(env.resource_block_allocation_matrix)\n",
        "                resource_allocation_constraint_violation_count.append(env.resource_allocation_constraint_violation)\n",
        "\n",
        "                policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "            # We evaluate the episode and we save the policy\n",
        "            if timesteps_since_eval >= eval_freq:\n",
        "                timesteps_since_eval %= eval_freq\n",
        "                evaluations.append(evaluate_policy(policy,env, access_point_id, user_association_number))\n",
        "                policy.save(file_name, directory=\"./pytorch_models/access_point_%d_user_association_%i\" % (access_point_id,user_association_number))\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_0), evaluations)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_1), timestep_rewards_energy_throughput)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_2), offload_actions)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_3), power_actions)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_4), subcarrier_actions)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_5), allocated_RBs)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_6), fairness_index)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_7), energy_efficiency_rewards)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_8), battery_energy_rewards)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_9), throughput_rewards)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_10), delay_rewards)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_11), sum_allocations_per_RB_matrix)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_12), RB_allocation_matrix)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_13), energy_rewards)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_14), delays)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_15), tasks_dropped)\n",
        "                #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_16), resource_allocation_matrix)\n",
        "                np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_17), resource_allocation_constraint_violation_count)\n",
        "\n",
        "            # When the training step is done, we reset the state of the environment\n",
        "            obs = env.reset()\n",
        "\n",
        "            # Set the Done to False\n",
        "            done = False\n",
        "\n",
        "            # Set rewards and episode timesteps to zero\n",
        "            episode_reward = 0\n",
        "            episode_timesteps = 0\n",
        "            episode_num += 1\n",
        "\n",
        "        # Before 10000 timesteps, we play random actions\n",
        "        if total_timesteps < start_timesteps:\n",
        "            if change_action == 0:\n",
        "                action = env.action_space.sample()\n",
        "                action = env.enforce_constraint(action)\n",
        "                change_action = 1\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "                action = env.enforce_constraint(action)\n",
        "                change_action = 0\n",
        "\n",
        "        else: # After 10000 timesteps, we switch to the model\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "            if expl_noise != 0:\n",
        "                action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape)).clip(env.action_space_low, env.action_space_high)\n",
        "\n",
        "            #print(action)\n",
        "            action = env.reshape_action_space_from_model_to_dict(action)\n",
        "            #action = env.enforce_constraint(action)\n",
        "            #print(action)\n",
        "\n",
        "\n",
        "        #print(\"Action in training\")\n",
        "        #print(action)\n",
        "        #print(' ')\n",
        "        # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "        new_obs, reward, dones, _ = env.step(action)\n",
        "        done = dones[len(dones) - 1]\n",
        "        # We check if the episode is done\n",
        "        done_bool = 0 if episode_timesteps + 1 == env.STEP_LIMIT else float(done)\n",
        "\n",
        "        # We increase the total reward\n",
        "        episode_reward += sum(reward)\n",
        "        #print('episode reward')\n",
        "        #print(episode_reward)\n",
        "        #episode_reward = interp(episode_reward,[720000000,863000000],[0,1000])\n",
        "        # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "        action = env.reshape_action_space_for_model(action)\n",
        "        replay_buffer.add((obs, new_obs, action, reward, dones[:len(dones)-1] + [done_bool]))\n",
        "\n",
        "        # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "        obs = new_obs\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1\n",
        "\n",
        "        # We add the last policy evaluation to our list of evaluations and we save our model\n",
        "    evaluations.append(evaluate_policy(policy,env, access_point_id, user_association_number))\n",
        "    if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models/access_point_%d_user_association_%i\" % (access_point_id,user_association_number))\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_0), evaluations)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_1), timestep_rewards_energy_throughput)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_2), offload_actions)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_3), power_actions)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_4), subcarrier_actions)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_5), allocated_RBs)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_6), fairness_index)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_7), energy_efficiency_rewards)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_8), battery_energy_rewards)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_9), throughput_rewards)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_10), delay_rewards)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_11), sum_allocations_per_RB_matrix)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_12), RB_allocation_matrix)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_13), energy_rewards)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_14), delays)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_15), tasks_dropped)\n",
        "    #np.save(\"./results/access_point_%d/%s\" % (access_point_id,file_name_16), resource_allocation_matrix)\n",
        "    np.save(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_17), resource_allocation_constraint_violation_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXgGnKD-yu5Q"
      },
      "source": [
        "\n",
        "# Calculate total reward across all access points at the end of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov6q4niCyxlJ"
      },
      "outputs": [],
      "source": [
        "def calculate_total_system_reward(num_access_point,user_association_number):\n",
        "   evaluations = []\n",
        "   access_point_id = 0\n",
        "   file_name_0 = \"Evaluations.npy\"\n",
        "   for x in range(1, num_access_point+1):\n",
        "      access_point_id = x\n",
        "      evaluation = np.load(\"./results/access_point_%d_user_association_%i/%s\" % (access_point_id,user_association_number,file_name_0))\n",
        "      evaluations.append(evaluation)\n",
        "\n",
        "   total_system_reward = 0\n",
        "\n",
        "   for evaluation in evaluations:\n",
        "      total_system_reward+= evaluation[len(evaluation)-1]\n",
        "   return total_system_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4ZrT-sy06B"
      },
      "source": [
        "\n",
        "# Prepare training data for user association DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO9HmxT_uBpL"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_training_DNN(all_users, num_users,num_access_points):\n",
        "   large_channel_gains = []\n",
        "   task_arrival_rates = []\n",
        "\n",
        "   for user in all_users:\n",
        "      large_channel_gains.append(user.initial_large_scale_gain_all_access_points(num_access_points))\n",
        "      task_arrival_rates.append(user.initial_arrival_rates())\n",
        "\n",
        "   large_channel_gains = np.array(large_channel_gains)\n",
        "   total_row_count = large_channel_gains.shape[0]\n",
        "   total_column_count = large_channel_gains.shape[1]\n",
        "   total_num_features = total_row_count*total_column_count\n",
        "   max_channel_gain = large_channel_gains.max()\n",
        "\n",
        "   task_arrival_rates = np.array(task_arrival_rates)\n",
        "   user_features = np.column_stack((large_channel_gains,task_arrival_rates))\n",
        "   total_row_count = user_features.shape[0]\n",
        "   total_column_count = user_features.shape[1]\n",
        "   total_num_features = total_row_count*total_column_count\n",
        "   user_features = np.transpose(user_features)\n",
        "   user_features = user_features.reshape(1,total_num_features)\n",
        "   user_features = user_features.squeeze()\n",
        "\n",
        "   return user_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfRB5dDZy5xe"
      },
      "source": [
        "# Prepare variables necessary to train the DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3aglcW5y9B2"
      },
      "outputs": [],
      "source": [
        "user_features = prepare_data_for_training_DNN(all_users, num_users,num_access_points)\n",
        "all_users, user_association_schemes, user_association_schemes_for_model_training = generate_user_association_schemes(all_users,num_users,num_access_points,num_batches)\n",
        "\n",
        "num_features = len(user_features)\n",
        "num_outputs = num_access_points*num_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKxibKTzzC8w"
      },
      "source": [
        "# Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clFUYqQLzHVM"
      },
      "outputs": [],
      "source": [
        "User_Association_DNN = UserAssociationDNN(input_dim=num_features, output_dim=num_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqu58iLAzLmr"
      },
      "source": [
        "# Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOiE7swVzMsI"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(User_Association_DNN.parameters(), lr=0.001)\n",
        "\n",
        "num_training_epochs = 10\n",
        "training_memory = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_user_association_scheme(user_association_epoch_number,total_system_rewards_user_associations):\n",
        "    access_points_envs = []\n",
        "    access_point_users = []\n",
        "    #user_association_epoch_number = i\n",
        "    policies = []\n",
        "    replay_buffers = []\n",
        "    threadlist = []\n",
        "\n",
        "    state_dim = 0\n",
        "    action_dim = 0\n",
        "    max_action = 0\n",
        "    seed = 0\n",
        "\n",
        "    for x in range(1,num_access_points+1):\n",
        "        access_point_users = find_access_point_users(all_users,x,user_association_epoch_number)\n",
        "        env_name = \"AccessPoint\" + str(x) + \"-v0\"\n",
        "        env = NetworkEnv(x,access_point_users,user_association_epoch_number)        \n",
        "        access_points_envs.append(env)\n",
        "\n",
        "\n",
        "    for x in range(0,num_access_points):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        state_dim = access_points_envs[x].observation_space.shape[1]\n",
        "        action_dim = access_points_envs[x].action_space_dim_1\n",
        "        max_action = float(access_points_envs[x].box_action_space.high[0][1]) # to change this soon\n",
        "\n",
        "        policy = TD3(state_dim, action_dim, max_action)\n",
        "        replay_buffer = ReplayBuffer()\n",
        "\n",
        "        policies.append(policy)\n",
        "        replay_buffers.append(replay_buffer)\n",
        "\n",
        "    # print('len of policies: ', len(policies))\n",
        "    # print('len of replay buffers: ', len(replay_buffers))\n",
        "    # print('len of access_point: ', len(access_points_envs))\n",
        "    # training(policies[1],replay_buffers[1],access_points_envs[1],1)\n",
        "    for x in range(0,num_access_points):\n",
        "        t = threading.Thread(target=training,args=(policies[x],replay_buffers[x],access_points_envs[x],x,user_association_epoch_number))\n",
        "        print('inner thread id: ', t.getName(), 'user_association number: ', user_association_epoch_number)\n",
        "        threadlist.append(t)\n",
        "        #t.start()\n",
        "    \n",
        "    for t in threadlist:\n",
        "        t.start()\n",
        "    for tr in threadlist:\n",
        "        tr.join()\n",
        "\n",
        "    print('ended training all threads')\n",
        "    total_system_reward = calculate_total_system_reward(num_access_points,user_association_epoch_number)\n",
        "    total_system_rewards_user_associations.append((total_system_reward,user_association_epoch_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YjtKU9JzQrY"
      },
      "source": [
        "# Start training the DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVKX4iAqzSK2"
      },
      "outputs": [],
      "source": [
        "for epoch in range(0,num_training_epochs):\n",
        "   if epoch != 0:\n",
        "      user_features = prepare_data_for_training_DNN(all_users, num_users,num_access_points)\n",
        "      all_users, user_association_schemes, user_association_schemes_for_model_training = generate_user_association_schemes(all_users,num_users,num_access_points,num_batches)\n",
        "\n",
        "   user_association_epoch_number = 0\n",
        "   total_system_reward = 0\n",
        "   total_system_rewards_user_associations = []\n",
        "   user_association_thread_list = []\n",
        "\n",
        "   for i in range(0,len(user_association_schemes_for_model_training)):\n",
        "      user_association_epoch_number = i\n",
        "      user_association_thread = threading.Thread(target=run_user_association_scheme,args=(user_association_epoch_number,total_system_rewards_user_associations))\n",
        "      print('outer thread id: ', user_association_thread.getName())\n",
        "      user_association_thread_list.append(user_association_thread)\n",
        "\n",
        "   for user_association_thread in user_association_thread_list:\n",
        "        user_association_thread.start()\n",
        "   for user_association_thread in user_association_thread_list:\n",
        "        user_association_thread.join()\n",
        "\n",
        "   total_system_rewards = []\n",
        "   user_association_ids = []\n",
        "\n",
        "   for total_system_rewards_user_association in total_system_rewards_user_associations:\n",
        "      total_system_rewards.append(total_system_rewards_user_association[0])\n",
        "      user_association_ids.append(total_system_rewards_user_association[1])\n",
        "      \n",
        "   total_system_rewards = np.array(total_system_rewards)\n",
        "   user_association_ids = np.array(user_association_ids)\n",
        "\n",
        "   max_index = np.array(total_system_rewards).argmax()\n",
        "   y_true = user_association_schemes_for_model_training[user_association_ids[max_index]]\n",
        "   training_memory.append((user_features,y_true))\n",
        "\n",
        "   user_features_for_training = []\n",
        "   y_true_values_for_training = []\n",
        "\n",
        "   for training_sample in training_memory:\n",
        "      user_features_for_training.append(training_sample[0])\n",
        "      y_true_values_for_training.append(training_sample[1])\n",
        "\n",
        "   user_features_for_training = np.array(user_features_for_training)\n",
        "   y_true_values_for_training = np.array(y_true_values_for_training)\n",
        "\n",
        "   user_features_for_training_tensor = torch.from_numpy(user_features_for_training)\n",
        "   y_true_values_for_training_tensor = torch.from_numpy(y_true_values_for_training)\n",
        "\n",
        "   y_pred_tensor = User_Association_DNN(user_features_for_training_tensor)\n",
        "\n",
        "   loss = criterion(y_pred_tensor, y_true_values_for_training_tensor)\n",
        "   optimizer.zero_grad()\n",
        "   loss.backward()\n",
        "   optimizer.step()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
