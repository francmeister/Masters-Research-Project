{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloning Github Repository and navigate into project folder\n",
        "!git clone https://github.com/francmeister/Masters-Research-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cd Masters-Research-Project/ICARTI-Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -e Network_Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cd Network_Env/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import Network_Env\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pygame\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self):\n",
        "    batch_size = len(self.storage)\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_actions = [], []\n",
        "    for i in ind: \n",
        "      state, action = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ObservationSpaceMemory(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states = []\n",
        "    for i in ind: \n",
        "      state, action = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "    return np.array(batch_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.sigmoid(self.layer_3(x))\n",
        "    #x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state).to(device)\n",
        "    #return self.actor(state).cpu().data.numpy().flatten()\n",
        "    return self.actor(state).cpu().data.numpy()\n",
        "\n",
        "  def train(self, memory, train_iterations):\n",
        "    \n",
        "    for it in range(train_iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, sâ€™, a, r) from the memory\n",
        "      batch_states, batch_actions = memory.sample()\n",
        "     # print(\"batch_states\")\n",
        "     # print(batch_states.shape)\n",
        "     # print(batch_states)\n",
        "     # print(\"batch_actions\")\n",
        "      #print(batch_actions.shape)\n",
        "      #print(batch_actions)\n",
        "      #batch_states = np.reshape(batch_states,(batch_states.shape[0]*batch_states.shape[1],batch_states.shape[2]))\n",
        "      #batch_actions = np.reshape(batch_actions,(batch_actions.shape[0]*batch_actions.shape[1],batch_actions.shape[2]))\n",
        "\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "\n",
        "      prediction = self.actor(state)\n",
        "\n",
        "      loss = F.mse_loss(prediction, action) \n",
        "      self.actor_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.actor_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_name = \"NetworkEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 7e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 10 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[1]\n",
        "action_dim = env.action_space.shape[1]\n",
        "max_action = float(env.action_space.high[0][1]) # to change this soon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = Memory()\n",
        "#obsSpaceMemory = ObservationSpaceMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Fmax = -10000000000\n",
        "epochs = 25\n",
        "sampling_frequency = 100\n",
        "train_iterations = 10\n",
        "obsSpaceMemory = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(1,epochs):\n",
        "    print(\"Epoch: \", i)\n",
        "    obsSpaceMemory = []\n",
        "    for i in range(1,sampling_frequency):\n",
        "        obsSample = env.observation_space.sample()\n",
        "        obsSpaceMemory.append(obsSample)\n",
        "\n",
        "    obsSpaceMemory = np.array(obsSpaceMemory)\n",
        "    obs = env.reset()\n",
        "    for obsSample in obsSpaceMemory:\n",
        "        #print(obsSample)\n",
        "        obsAction = policy.select_action(obsSample)\n",
        "        obs, rewards, done, _ = env.step(obsAction)\n",
        "\n",
        "        index = 0\n",
        "        for reward in rewards:\n",
        "            if reward > Fmax:\n",
        "                memory.add((obsSample[index],obsAction[index]))\n",
        "                Fmax = reward\n",
        "                print(\"Fmax\")\n",
        "                print(Fmax)\n",
        "            index+=1\n",
        "\n",
        "    if len(memory.storage) > 0:\n",
        "        policy.train(memory,train_iterations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TD3_Ant.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
